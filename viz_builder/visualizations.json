{
  "daa78010-c289-40ea-80cb-dc4f0d05b376": {
    "id": "daa78010-c289-40ea-80cb-dc4f0d05b376",
    "created_at": "2025-04-07T15:37:17.833679",
    "updated_at": "2025-04-07T15:37:17.833679",
    "code": "import streamlit as st\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport scipy\nimport math\nimport random\nimport datetime\nimport time\nimport json\nimport re\nimport os\nimport sys\nimport io\nimport collections\nimport itertools\nimport sklearn\nimport statsmodels\nimport statsmodels.api as sm\nimport altair as alt\n\nst.title(\"Black-Scholes Option Pricing Simulation\")\n\n# Input parameters\nstock_price = st.number_input(\"Current Stock Price (S)\", value=100.0, min_value=0.0)\nstrike_price = st.number_input(\"Strike Price (K)\", value=100.0, min_value=0.0)\ntime_to_expiration = st.number_input(\"Time to Expiration (T in years)\", value=1.0, min_value=0.0)\nrisk_free_rate = st.number_input(\"Risk-Free Interest Rate (r)\", value=0.05, min_value=0.0)\nvolatility = st.number_input(\"Volatility (\u03c3)\", value=0.20, min_value=0.0)\nnumber_of_simulations = st.number_input(\"Number of Simulations\", value=1000, min_value=1)\n\n# Black-Scholes Formula (Call Option)\ndef black_scholes_call(S, K, T, r, sigma):\n    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    call_price = S * scipy.stats.norm.cdf(d1) - K * np.exp(-r * T) * scipy.stats.norm.cdf(d2)\n    return call_price\n\n# Calculate Black-Scholes price\ncall_price = black_scholes_call(stock_price, strike_price, time_to_expiration, risk_free_rate, volatility)\nst.write(f\"Black-Scholes Call Option Price: {call_price:.2f}\")\n\n# Simulate stock prices\nnp.random.seed(42) # for reproducibility\nz = np.random.standard_normal(number_of_simulations)\nST = stock_price * np.exp((risk_free_rate - 0.5 * volatility ** 2) * time_to_expiration + volatility * np.sqrt(time_to_expiration) * z)\n\n# Calculate option payoffs\npayoffs = np.maximum(ST - strike_price, 0)\n\n# Discount payoffs to present value\ndiscount_factor = np.exp(-risk_free_rate * time_to_expiration)\noption_prices = discount_factor * payoffs\n\n# Monte Carlo estimate of option price\nmonte_carlo_price = np.mean(option_prices)\nst.write(f\"Monte Carlo Estimated Call Option Price: {monte_carlo_price:.2f}\")\n\n# Histogram of simulated option prices\nfig, ax = plt.subplots()\nax.hist(option_prices, bins=50, alpha=0.7, label='Simulated Option Prices')\nax.axvline(monte_carlo_price, color='red', linestyle='dashed', linewidth=1, label=f'Monte Carlo Price: {monte_carlo_price:.2f}')\nax.axvline(call_price, color='green', linestyle='dashed', linewidth=1, label=f'Black-Scholes Price: {call_price:.2f}')\nax.set_xlabel('Option Price')\nax.set_ylabel('Frequency')\nax.set_title('Monte Carlo Simulation of Call Option Prices')\nax.legend()\nst.pyplot(fig)\n\n# Distribution of simulated stock prices at expiration\nfig2, ax2 = plt.subplots()\nax2.hist(ST, bins=50, alpha=0.7, label='Simulated Stock Prices at Expiration')\nax2.axvline(strike_price, color='red', linestyle='dashed', linewidth=1, label=f'Strike Price: {strike_price:.2f}')\nax2.set_xlabel('Stock Price at Expiration')\nax2.set_ylabel('Frequency')\nax2.set_title('Monte Carlo Simulation of Stock Prices at Expiration')\nax2.legend()\nst.pyplot(fig2)\n\n# Sensitivity analysis (Greeks - approximate using small changes)\n\ndelta_h = 0.01 * stock_price\nstock_price_up = stock_price + delta_h\ncall_price_up = black_scholes_call(stock_price_up, strike_price, time_to_expiration, risk_free_rate, volatility)\ndelta = (call_price_up - call_price) / delta_h\n\nvega_h = 0.01\nvolatility_up = volatility + vega_h\ncall_price_vega_up = black_scholes_call(stock_price, strike_price, time_to_expiration, risk_free_rate, volatility_up)\nvega = (call_price_vega_up - call_price) / vega_h\n\nst.subheader(\"Option Greeks (Approximation)\")\nst.write(f\"Delta: {delta:.4f}\")\nst.write(f\"Vega: {vega:.4f}\")",
    "title": "Black-Scholes & Monte Carlo Option Pricing Simulator",
    "description": "",
    "tags": []
  },
  "fab0913a-4a36-4982-ad2b-050db990cc73": {
    "id": "fab0913a-4a36-4982-ad2b-050db990cc73",
    "created_at": "2025-04-07T15:38:02.114691",
    "updated_at": "2025-04-07T15:38:02.114691",
    "code": "import streamlit as st\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport scipy\nimport math\nimport random\nimport datetime\nimport time\nimport json\nimport re\nimport os\nimport sys\nimport io\nimport collections\nimport itertools\nimport sklearn\nimport statsmodels\nimport statsmodels.api as sm\nimport altair as alt\n\nst.title(\"SVM with Non-Linear Data and Kernels\")\n\n# Generate sample non-linear data\nn_samples = st.slider(\"Number of samples:\", min_value=50, max_value=500, value=200, step=10)\nrandom_state = 42\nrng = np.random.RandomState(random_state)\nX = rng.randn(n_samples, 2)\ny = np.logical_xor(X[:, 0] > 0, X[:, 1] > 0)\ny = np.where(y, 1, 0)\n\ndf = pd.DataFrame(X, columns=['Feature 1', 'Feature 2'])\ndf['Target'] = y\n\n# Kernel selection\nkernel_options = ['linear', 'poly', 'rbf', 'sigmoid']\nselected_kernel = st.selectbox(\"Select Kernel:\", kernel_options)\n\n# SVM parameters (common to all kernels, with defaults)\nC = st.slider(\"Regularization (C):\", min_value=0.1, max_value=10.0, value=1.0, step=0.1)\n\n# Parameter specific to polynomial kernel\nif selected_kernel == 'poly':\n    degree = st.slider(\"Degree (for Poly Kernel):\", min_value=2, max_value=10, value=3, step=1)\n    clf = sklearn.svm.SVC(kernel=selected_kernel, C=C, degree=degree, random_state=random_state)\n# Parameter specific to rbf kernel\nelif selected_kernel == 'rbf':\n    gamma = st.slider(\"Gamma (for RBF Kernel):\", min_value=0.01, max_value=5.0, value=0.5, step=0.01)\n    clf = sklearn.svm.SVC(kernel=selected_kernel, C=C, gamma=gamma, random_state=random_state)\n# Parameter specific to sigmoid kernel\nelif selected_kernel == 'sigmoid':\n    gamma = st.slider(\"Gamma (for Sigmoid Kernel):\", min_value=0.01, max_value=5.0, value=0.5, step=0.01)\n    coef0 = st.slider(\"Coef0 (for Sigmoid Kernel):\", min_value=-5.0, max_value=5.0, value=0.0, step=0.1)\n    clf = sklearn.svm.SVC(kernel=selected_kernel, C=C, gamma=gamma, coef0=coef0, random_state=random_state)\nelse:\n    clf = sklearn.svm.SVC(kernel=selected_kernel, C=C, random_state=random_state)\n\n# Train the SVM model\nclf.fit(X, y)\n\n# Create a meshgrid for plotting the decision boundary\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\n\n# Predict the class for each point in the meshgrid\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plotting using matplotlib\nfig, ax = plt.subplots(figsize=(8, 6))\nax.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.5)\nax.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu, edgecolors='k')\nax.set_title(f\"SVM with {selected_kernel} kernel\")\nax.set_xlabel(\"Feature 1\")\nax.set_ylabel(\"Feature 2\")\n\n# Display the plot in Streamlit\nst.pyplot(fig)\n\n# Display model performance\ny_pred = clf.predict(X)\naccuracy = sklearn.metrics.accuracy_score(y, y_pred)\nst.write(f\"Accuracy: {accuracy:.2f}\")",
    "title": "SVM Kernel Explorer: Non-Linear Data Classification",
    "description": "",
    "tags": []
  },
  "3cef6bea-4170-4f0e-931d-248f4be67ecf": {
    "id": "3cef6bea-4170-4f0e-931d-248f4be67ecf",
    "created_at": "2025-04-07T19:00:22.170747",
    "updated_at": "2025-04-07T19:01:43.473153",
    "code": "import streamlit as st\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport scipy\nimport math\nimport random\nimport datetime\nimport time\nimport json\nimport re\nimport os\nimport sys\nimport io\nimport collections\nimport itertools\nimport sklearn\nimport statsmodels\nimport statsmodels.api as sm\nimport altair as alt\nimport pygame\n\nst.title(\"Interactive Brownian Motion Simulation\")\n\n# Simulation parameters\nnum_particles = st.slider(\"Number of Particles\", min_value=1, max_value=100, value=20, step=1)\nnum_steps = st.slider(\"Number of Steps\", min_value=10, max_value=500, value=100, step=10)\nstep_size = st.slider(\"Step Size\", min_value=0.01, max_value=1.0, value=0.1, step=0.01)\ndisplay_trails = st.checkbox(\"Display Trails\", value=True)\narena_size = st.slider(\"Arena Size\", min_value=10, max_value=100, value=50, step=5)\n\nrun_simulation = st.button(\"Run Simulation\")\n\nif run_simulation:\n    # Initialize particle positions\n    x_positions = np.zeros((num_particles, num_steps))\n    y_positions = np.zeros((num_particles, num_steps))\n    x_positions[:, 0] = np.random.uniform(low=-arena_size/2, high=arena_size/2, size=num_particles)\n    y_positions[:, 0] = np.random.uniform(low=-arena_size/2, high=arena_size/2, size=num_particles)\n\n    # Create a placeholder for the plot\n    plot_placeholder = st.empty()\n\n    # Simulate Brownian motion\n    for j in range(1, num_steps):\n        for i in range(num_particles):\n            angle = np.random.uniform(0, 2 * np.pi)\n            dx = step_size * np.cos(angle)\n            dy = step_size * np.sin(angle)\n\n            x_positions[i, j] = x_positions[i, j-1] + dx\n            y_positions[i, j] = y_positions[i, j-1] + dy\n\n            # Boundary conditions (reflective)\n            if x_positions[i, j] > arena_size/2:\n                x_positions[i, j] = arena_size/2 - (x_positions[i, j] - arena_size/2)\n            if x_positions[i, j] < -arena_size/2:\n                x_positions[i, j] = -arena_size/2 + (-arena_size/2 - x_positions[i, j])\n            if y_positions[i, j] > arena_size/2:\n                y_positions[i, j] = arena_size/2 - (y_positions[i, j] - arena_size/2)\n            if y_positions[i, j] < -arena_size/2:\n                y_positions[i, j] = -arena_size/2 + (-arena_size/2 - y_positions[i, j])\n\n        # Plot the results\n        fig, ax = plt.subplots()\n        ax.set_xlim(-arena_size/2, arena_size/2)\n        ax.set_ylim(-arena_size/2, arena_size/2)\n        ax.set_xlabel(\"X Position\")\n        ax.set_ylabel(\"Y Position\")\n        ax.set_title(\"Brownian Motion Simulation\")\n        ax.grid(True)\n\n        if display_trails:\n            for i in range(num_particles):\n                ax.plot(x_positions[i, :j], y_positions[i, :j], alpha=0.5) # Plot trails\n        else:\n            for i in range(num_particles):\n                ax.plot(x_positions[i, j], y_positions[i, j], 'o') # Plot only the end points\n\n        plot_placeholder.pyplot(fig)\n        time.sleep(0.01)\n\n    # Display a histogram of final positions\n    final_x = x_positions[:, -1]\n    final_y = y_positions[:, -1]\n\n    fig_hist = px.histogram(x=final_x, nbins=20, title=\"Distribution of Final X Positions\")\n    st.plotly_chart(fig_hist)\n\n    fig_hist_y = px.histogram(x=final_y, nbins=20, title=\"Distribution of Final Y Positions\")\n    st.plotly_chart(fig_hist_y)",
    "title": "Interactive Brownian Motion Simulation with Position Histograms",
    "description": "",
    "tags": []
  },
  "c6fd2075-7abc-4429-bf8f-4d23d9e07542": {
    "id": "c6fd2075-7abc-4429-bf8f-4d23d9e07542",
    "created_at": "2025-04-07T19:02:36.811449",
    "updated_at": "2025-04-07T19:02:36.811449",
    "code": "import streamlit as st\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport scipy\nimport math\nimport random\nimport datetime\nimport time\nimport json\nimport re\nimport os\nimport sys\nimport io\nimport collections\nimport itertools\nimport sklearn\nimport statsmodels\nimport statsmodels.api as sm\nimport altair as alt\nimport pygame\n\nfrom sklearn.datasets import make_moons, make_circles\nfrom sklearn.preprocessing import StandardScaler\nimport umap\n\nst.title(\"UMAP Visualization on Multiple Non-Linear Datasets\")\n\n# Sidebar for parameters\nst.sidebar.header(\"UMAP Parameters\")\nn_neighbors = st.sidebar.slider(\"Number of Neighbors\", min_value=2, max_value=200, value=15, step=1)\nmin_dist = st.sidebar.slider(\"Minimum Distance\", min_value=0.0, max_value=1.0, value=0.1, step=0.01)\nn_components = st.sidebar.slider(\"Number of Components\", min_value=2, max_value=3, value=2, step=1) # Allow 3 components\nrandom_state = st.sidebar.number_input(\"Random State\", value=42, step=1)\n\n# Dataset selection\ndataset_choice = st.sidebar.selectbox(\"Choose a Dataset\", [\"Moons\", \"Circles\", \"Custom\"])\n\n# Data generation\nif dataset_choice == \"Moons\":\n    X, y = make_moons(n_samples=200, noise=0.05, random_state=random_state)\n    dataset_name = \"Moons Dataset\"\nelif dataset_choice == \"Circles\":\n    X, y = make_circles(n_samples=200, factor=0.5, noise=0.05, random_state=random_state)\n    dataset_name = \"Circles Dataset\"\nelse:  # Custom Dataset\n    st.sidebar.header(\"Custom Dataset Parameters\")\n    n_samples = st.sidebar.slider(\"Number of Samples\", min_value=50, max_value=500, value=200, step=1)\n    n_features = st.sidebar.slider(\"Number of Features\", min_value=2, max_value=10, value=2, step=1)\n    random_state_custom = st.sidebar.number_input(\"Custom Random State\", value=42, step=1, key='custom_random')\n    X = np.random.rand(n_samples, n_features)\n    y = np.random.randint(0, 2, n_samples)  # Generate some random labels\n    dataset_name = \"Custom Dataset\"\n\n\n# Scale the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# UMAP Embedding\nreducer = umap.UMAP(n_neighbors=n_neighbors,\n                  min_dist=min_dist,\n                  n_components=n_components,\n                  random_state=random_state)\n\nembedding = reducer.fit_transform(X_scaled)\n\n# Visualization\nst.header(f\"UMAP Embedding of {dataset_name}\")\n\nif n_components == 2:\n    fig = px.scatter(x=embedding[:, 0], y=embedding[:, 1], color=y.astype(str),\n                     labels={'x': 'UMAP Component 1', 'y': 'UMAP Component 2', 'color': 'Cluster'},\n                     title=f\"UMAP Projection with n_neighbors={n_neighbors}, min_dist={min_dist}\")\n    st.plotly_chart(fig)\nelif n_components == 3:\n    fig = px.scatter_3d(x=embedding[:, 0], y=embedding[:, 1], z=embedding[:, 2], color=y.astype(str),\n                        labels={'x': 'UMAP Component 1', 'y': 'UMAP Component 2', 'z': 'UMAP Component 3', 'color': 'Cluster'},\n                        title=f\"UMAP Projection with n_neighbors={n_neighbors}, min_dist={min_dist}\")\n    st.plotly_chart(fig)\n\n\n# Display the original data (optional)\nst.subheader(\"Original Data (First 5 rows)\")\nif dataset_choice == \"Custom\":\n    df = pd.DataFrame(X)\nelse:\n    df = pd.DataFrame(X, columns=[f\"Feature {i+1}\" for i in range(X.shape[1])])\ndf['target'] = y\nst.dataframe(df.head())\n\nst.subheader(\"UMAP Parameters Used:\")\nst.write(f\"Number of Neighbors: {n_neighbors}\")\nst.write(f\"Minimum Distance: {min_dist}\")\nst.write(f\"Number of Components: {n_components}\")\nst.write(f\"Random State: {random_state}\")",
    "title": "Interactive UMAP Explorer: Datasets and Parameter Tuning",
    "description": "",
    "tags": []
  },
  "b0739a1a-ca0a-41fa-9adb-dc05f84d6b85": {
    "id": "b0739a1a-ca0a-41fa-9adb-dc05f84d6b85",
    "created_at": "2025-04-07T19:05:22.433085",
    "updated_at": "2025-04-07T19:07:23.789952",
    "code": "import streamlit as st\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport scipy\nimport math\nimport random\nimport datetime\nimport time\nimport json\nimport re\nimport os\nimport sys\nimport io\nimport collections\nimport itertools\nimport sklearn\nimport statsmodels\nimport statsmodels.api as sm\nimport altair as alt\nimport pygame\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom PIL import Image\n\nst.title(\"Image Compression with PCA\")\n\nst.write(\"This app demonstrates image compression using Principal Component Analysis (PCA). PCA reduces the dimensionality of the image data, allowing for smaller file sizes with some loss of detail.\")\n\n# Image Upload\nuploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n\nif uploaded_file is not None:\n    image = Image.open(uploaded_file)\n    st.image(image, caption=\"Original Image\", use_column_width=True)\n\n    # Convert image to numpy array\n    img_array = np.array(image)\n\n    # Convert to Grayscale\n    if len(img_array.shape) == 3:\n        img_gray = np.dot(img_array[...,:3], [0.2989, 0.5870, 0.1140])\n    else:\n        img_gray = img_array\n\n    img_gray = img_gray.astype(np.uint8)\n\n    st.image(img_gray, caption=\"Grayscale Image\", use_column_width=True)\n    st.write(f\"Grayscale Image Shape: {img_gray.shape}\")\n\n    # Display the matrix of grayscale values\n    st.write(\"Grayscale Value Matrix:\")\n    st.dataframe(pd.DataFrame(img_gray))\n\n    # Number of components\n    n_components = st.slider(\"Number of Principal Components\", 1, img_gray.shape[1], img_gray.shape[1]//10)\n\n    # Reshape image into a matrix where each row is a pixel\n    img_matrix = img_gray\n\n    # Standardize the data\n    scaler = StandardScaler()\n    scaled_data = scaler.fit_transform(img_matrix)\n\n    # Apply PCA\n    pca = PCA(n_components=n_components)\n    pca.fit(scaled_data)\n    transformed_data = pca.transform(scaled_data)\n\n    # Inverse transform\n    inverse_data = pca.inverse_transform(transformed_data)\n\n    # Scale back to original range\n    inverse_data = scaler.inverse_transform(inverse_data)\n\n    # Clip values to be within the valid range\n    compressed_img = np.clip(inverse_data, 0, 255).astype(np.uint8)\n\n    # Display compressed image\n    st.image(compressed_img, caption=f\"Compressed Image with {n_components} Components\", use_column_width=True)\n\n    # Explained variance ratio\n    explained_variance = pca.explained_variance_ratio_\n    explained_variance_cumulative = np.cumsum(explained_variance)\n\n    # Plot explained variance\n    fig, ax = plt.subplots()\n    ax.plot(explained_variance_cumulative)\n    ax.set_xlabel(\"Number of Components\")\n    ax.set_ylabel(\"Cumulative Explained Variance\")\n    ax.set_title(\"Explained Variance vs. Number of Components\")\n    st.pyplot(fig)\n\n    # Display explained variance ratio data\n    variance_df = pd.DataFrame({'Component': range(1, len(explained_variance) + 1),\n                                  'Explained Variance': explained_variance,\n                                  'Cumulative Explained Variance': explained_variance_cumulative})\n    st.dataframe(variance_df)\n\n    # Compression Ratio\n    original_size = os.path.getsize(uploaded_file)\n    compressed_size = compressed_img.nbytes\n    compression_ratio = original_size / compressed_size\n\n    st.write(f\"Original File Size: {original_size} bytes\")\n    st.write(f\"Compressed Image Size: {compressed_size} bytes\")\n    st.write(f\"Compression Ratio: {compression_ratio:.2f}\")\n\n    if st.button(\"Rerun\"):\n        st.rerun()\nelse:\n    st.write(\"Please upload an image to begin.\")",
    "title": "PCA Image Compression Visualization",
    "description": "",
    "tags": []
  },
  "7595b588-78f5-46b7-9f58-09a985f577c9": {
    "id": "7595b588-78f5-46b7-9f58-09a985f577c9",
    "created_at": "2025-04-07T19:11:36.069772",
    "updated_at": "2025-04-07T19:11:36.069772",
    "code": "import streamlit as st\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nimport scipy\nimport math\nimport random\nimport datetime\nimport time\nimport json\nimport re\nimport os\nimport sys\nimport io\nimport collections\nimport itertools\nimport sklearn\nimport statsmodels\nimport statsmodels.api as sm\nimport altair as alt\nimport pygame\n\nst.title(\"Neural Network Function Approximator\")\n\n# Generate sample data\nnp.random.seed(42)\nnum_samples = 100\nx = np.linspace(-5, 5, num_samples)\n# Target function: a sine wave with some noise\ny_true = np.sin(x) + np.random.normal(0, 0.2, num_samples)\n\n# Neural network parameters (simplified for demonstration)\nst.sidebar.header(\"Neural Network Parameters\")\nlearning_rate = st.sidebar.slider(\"Learning Rate\", 0.001, 0.1, 0.01)\nnum_epochs = st.sidebar.slider(\"Number of Epochs\", 1, 100, 20)\nhidden_units = st.sidebar.slider(\"Hidden Units\", 1, 20, 5)\n\n# Initialize weights and biases randomly\nw1 = np.random.randn(1, hidden_units) * 0.1\nb1 = np.zeros((1, hidden_units))\nw2 = np.random.randn(hidden_units, 1) * 0.1\nb2 = np.zeros((1, 1))\n\n# Training loop (very basic)\nfor epoch in range(num_epochs):\n    # Forward pass\n    z1 = x.reshape(-1, 1) @ w1 + b1\n    a1 = np.tanh(z1)  # Activation function: tanh\n    z2 = a1 @ w2 + b2\n    y_pred = z2.flatten() # Linear output\n\n    # Calculate loss (Mean Squared Error)\n    loss = np.mean((y_pred - y_true)**2)\n\n    # Backpropagation (simplified gradient descent)\n    dz2 = (y_pred - y_true).reshape(-1, 1)\n    dw2 = a1.T @ dz2 / num_samples\n    db2 = np.sum(dz2, axis=0, keepdims=True) / num_samples\n    da1 = dz2 @ w2.T\n    dz1 = da1 * (1 - np.tanh(z1)**2)  # Derivative of tanh\n    dw1 = x.reshape(-1, 1).T @ dz1 / num_samples\n    db1 = np.sum(dz1, axis=0, keepdims=True) / num_samples\n\n    # Update parameters\n    w1 -= learning_rate * dw1\n    b1 -= learning_rate * db1\n    w2 -= learning_rate * dw2\n    b2 -= learning_rate * db2\n\n    # Plot the results\n    fig, ax = plt.subplots()\n    ax.scatter(x, y_true, label=\"True Function\", alpha=0.7)\n    ax.plot(x, y_pred, color='red', label=\"Approximated Function\")\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_title(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n    ax.legend()\n    st.pyplot(fig)\n    #time.sleep(0.01) # Slow down for better visualization (optional)\n    #st.rerun()",
    "title": "Neural Network Function Approximation Visualization",
    "description": "",
    "tags": []
  }
}