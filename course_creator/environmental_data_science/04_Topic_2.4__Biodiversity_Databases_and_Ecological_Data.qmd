---
title: "Biodiversity Databases and Ecological Data"
subtitle: "Environmental Science - Lecture 2: Data Types and Sources in Environmental Science"
bibliography: biodiversity_databases.bib
---

## Topic Overview ðŸŒ¿

Biodiversity databases and ecological data represent a cornerstone of modern environmental science, providing the empirical foundation for understanding species distributions, ecosystem health, and global biodiversity patterns. In an era of unprecedented environmental change, the systematic collection, organization, and analysis of biodiversity information has become critical for conservation planning, policy development, and scientific research.

This topic explores the diverse landscape of biodiversity information systems, ranging from traditional museum collections digitized for the 21st century to cutting-edge citizen science platforms that harness the power of community engagement. We examine how species occurrence dataâ€”records of where and when organisms have been observedâ€”are collected, standardized, and made accessible to researchers worldwide. These databases aggregate millions of observations from field surveys, monitoring networks, specimen collections, and increasingly, from citizen scientists armed with smartphones and field guides.

The importance of standardized ecological data formats cannot be overstated. Just as a common language enables communication across cultures, standardized data formats like Darwin Core enable biodiversity data from disparate sources to be integrated, compared, and analyzed at scales from local to global. This interoperability has revolutionized our ability to track species distributions over time, identify biodiversity hotspots, detect range shifts associated with climate change, and assess ecosystem health across landscapes.

**Relevance within Environmental Science:**

Within the broader context of environmental science and this lecture on data types and sources, biodiversity databases serve as a critical complement to remote sensing imagery, sensor networks, and climate records. While satellites can detect land cover change and sensors can measure water quality, biodiversity databases provide the biological contextâ€”which species are present, how populations are changing, and how ecological communities are responding to environmental stressors. This biological dimension is essential for holistic environmental assessment and management.

Furthermore, biodiversity data exemplifies many of the data quality, metadata, and documentation challenges discussed in subsequent topics of this lecture. Species occurrence records vary enormously in spatial precision, temporal resolution, taxonomic accuracy, and sampling effortâ€”all factors that must be carefully considered when analyzing these data. Understanding how to access, evaluate, and utilize biodiversity databases is therefore fundamental training for environmental scientists working across scales from local conservation projects to global change research.

## Background & Theory ðŸ“š

### Historical Context: From Cabinets of Curiosity to Digital Databases

The systematic documentation of biodiversity has ancient roots, but the modern era of biodiversity databases emerged from centuries of natural history collections. Early naturalists like Carl Linnaeus (1707-1778) established taxonomic frameworks that remain foundational today. The 18th and 19th centuries saw the establishment of major natural history museums whose collectionsâ€”preserved specimens, field notes, and illustrationsâ€”represent irreplaceable historical records of species distributions and ecosystem composition.

The digital revolution transformed these static collections into dynamic research resources. Beginning in the 1990s, museums and herbaria began digitizing specimen records, creating databases that could be searched, analyzed, and shared electronically. This digitization effort accelerated dramatically in the 21st century, driven by declining biodiversity, advances in database technology, and recognition that integrated biodiversity data could address pressing conservation questions [@graham2004].

Parallel to museum digitization, ecological monitoring networks emerged to track biodiversity changes systematically. Long-term ecological research (LTER) sites, established in the United States in 1980 and subsequently adopted internationally, provided standardized, sustained observations of ecosystems. These networks generated time-series data essential for detecting trends and understanding ecological dynamics [@lindenmayer2012].

### Major Biodiversity Database Types and Platforms

#### 1. Species Occurrence Databases

Species occurrence databases document where and when organisms have been observed or collected. The Global Biodiversity Information Facility (GBIF) represents the largest and most comprehensive aggregator, providing access to over 2 billion occurrence records from thousands of contributing institutions worldwide. GBIF operates on principles of open access, standardization, and federationâ€”individual institutions maintain their data but make it discoverable through a unified portal [@gbif2021].

**Key characteristics of occurrence data:**

- **Spatial information**: Geographic coordinates (latitude/longitude) or locality descriptions
- **Temporal information**: Date of observation or collection
- **Taxonomic information**: Species identification, ideally to species level
- **Basis of record**: Whether the record represents a preserved specimen, human observation, machine observation, or other evidence type
- **Data quality indicators**: Coordinate precision, identification confidence, data validation flags

Other significant occurrence databases include:

- **VertNet**: Focused on vertebrate specimen records from natural history collections
- **Ocean Biodiversity Information System (OBIS)**: Marine and coastal species occurrences
- **iDigBio**: Integrated Digitized Biocollections, primarily focused on North American specimens

#### 2. Citizen Science Platforms

Citizen science has democratized biodiversity data collection, enabling millions of observers to contribute scientifically valuable observations. These platforms leverage smartphone technology, automated species identification, and community expertise to generate massive datasets that would be impossible through traditional scientific surveys alone [@sullivan2014].

**eBird**, managed by the Cornell Lab of Ornithology, exemplifies the power of citizen science for biodiversity monitoring. Launched in 2002, eBird has accumulated over 1 billion bird observations, providing unprecedented insights into bird distributions, migration patterns, and population trends. The platform incorporates sophisticated data quality filters and statistical models to account for observer effort and expertise variation [@sullivan2009].

**iNaturalist**, a joint initiative of the California Academy of Sciences and National Geographic Society, extends the citizen science model across all taxonomic groups. Users upload photos of organisms, which are identified through a combination of computer vision algorithms and community expertise. Observations achieving "Research Grade" status (confirmed by multiple independent identifiers) contribute to GBIF and scientific research.

Other notable platforms include:

- **eBird**: Bird observations globally
- **iNaturalist**: All taxa, with strong community identification support
- **Pl@ntNet**: Plant identification through image recognition
- **Zooniverse**: Crowdsourced classification of images, including camera trap photos

#### 3. Museum Collections and Natural History Data

Natural history collections represent centuries of biodiversity documentation, with specimens serving as physical vouchers that can be re-examined as taxonomic understanding evolves or new analytical techniques emerge. Modern digitization efforts capture not only specimen data but also label information, field notes, and increasingly, high-resolution specimen images [@beaman2018].

**Types of collection data:**

- **Specimen records**: Preserved organisms (taxidermy, dried specimens, fluid-preserved)
- **Tissue samples**: Genetic material for molecular analysis
- **Observation records**: Historical field notes and surveys
- **Associated data**: Environmental conditions, habitat descriptions, associated species

Digitized collections enable novel research applications. For example, herbarium specimens can be used to study phenological shifts over decades, while genetic material from historical specimens can reveal evolutionary responses to environmental change.

#### 4. Ecological Monitoring Networks

Systematic, long-term monitoring networks provide standardized biodiversity data with consistent methodology over timeâ€”essential for detecting trends and understanding ecological processes [@lindenmayer2012].

**Examples of monitoring networks:**

- **Long-Term Ecological Research (LTER) Network**: Standardized ecosystem monitoring across diverse biomes
- **National Ecological Observatory Network (NEON)**: Continental-scale ecological observation system in the United States
- **eLTER**: European Long-Term Ecosystem Research network
- **Breeding Bird Survey**: Continent-wide bird monitoring in North America
- **Butterfly Monitoring Schemes**: Standardized butterfly transects across Europe and beyond

These networks typically employ rigorous sampling protocols, quality control procedures, and sustained funding, making their data particularly valuable for trend analysis and hypothesis testing.

### Standardized Ecological Data Formats

Data standardization is fundamental to biodiversity informatics, enabling integration of heterogeneous data from diverse sources. Several key standards have emerged:

#### Darwin Core

Darwin Core is the most widely adopted standard for biodiversity data, developed by the Biodiversity Information Standards (TDWG) organization. It defines a vocabulary of terms for describing species occurrences and related information [@wieczorek2012].

**Core Darwin Core terms include:**

- **Occurrence terms**: `occurrenceID`, `basisOfRecord`, `recordedBy`, `individualCount`
- **Location terms**: `decimalLatitude`, `decimalLongitude`, `coordinateUncertaintyInMeters`, `locality`
- **Taxonomic terms**: `scientificName`, `kingdom`, `phylum`, `class`, `order`, `family`, `genus`, `specificEpithet`
- **Temporal terms**: `eventDate`, `year`, `month`, `day`
- **Identification terms**: `identifiedBy`, `dateIdentified`, `identificationQualifier`

Darwin Core's flexibility allows it to accommodate diverse data types while maintaining core interoperability. Extensions handle specialized data like measurements, multimedia, or resource relationships.

#### Ecological Metadata Language (EML)

EML provides a standardized format for documenting ecological datasets, ensuring that data remain interpretable and usable long after collection. EML documents describe:

- **Dataset identity**: Title, creators, publication date
- **Data structure**: Variables, units, missing value codes
- **Collection methods**: Sampling protocols, equipment, study design
- **Geographic coverage**: Bounding coordinates, site descriptions
- **Temporal coverage**: Date ranges, sampling frequency
- **Taxonomic coverage**: Species or groups studied

#### Other Relevant Standards

- **Simple Darwin Core**: Simplified Darwin Core for basic occurrence data
- **Access to Biological Collection Data (ABCD)**: European standard for collection data
- **Minimum Information about any (x) Sequence (MIxS)**: Standards for genomic and metagenomic data
- **Observation Data Model (ODM)**: For sensor-based environmental observations

### Data Quality Considerations in Biodiversity Databases

Biodiversity data quality varies enormously, influenced by collection methods, observer expertise, technological capabilities, and documentation practices. Understanding and addressing quality issues is essential for robust analysis [@chapman2005].

#### Spatial Uncertainty

Geographic coordinates may have varying precision:

- **High precision**: GPS coordinates accurate to meters
- **Moderate precision**: Locality descriptions georeferenced to landmarks
- **Low precision**: County or regional-level locations
- **Errors**: Coordinate swaps (latitude/longitude reversed), datum mismatches, transcription errors

The `coordinateUncertaintyInMeters` field in Darwin Core helps quantify spatial precision, but many historical records lack this information. Georeferencing protocols provide guidelines for assigning coordinates to locality descriptions and estimating uncertainty.

#### Taxonomic Uncertainty

Species identifications vary in reliability:

- **Expert determination**: Specimens identified by taxonomic specialists
- **Field identification**: Observations by trained naturalists
- **Crowd-sourced identification**: Community consensus on platforms like iNaturalist
- **Automated identification**: Computer vision algorithms with varying accuracy

Taxonomic concepts also change over time as species are split, lumped, or reclassified. Taxonomic name resolution services help standardize names and track synonymy.

#### Temporal Precision

Dates may be recorded at different resolutions (year only, month and year, exact date) or may be uncertain for historical specimens. The `eventDate` field in Darwin Core uses ISO 8601 format to accommodate varying temporal precision.

#### Sampling Bias

Biodiversity data are inherently biased by:

- **Geographic bias**: Oversampling near roads, cities, and research stations
- **Taxonomic bias**: Charismatic species better documented than cryptic or rare species
- **Temporal bias**: Seasonal variation in sampling effort
- **Observer bias**: Varying expertise and effort among contributors

Statistical methods like spatial filtering, rarefaction, and occupancy modeling can help address sampling bias, but understanding bias patterns is essential for interpretation.

### Conceptual Framework: From Data to Knowledge

```{mermaid}
flowchart TD
    A[Biodiversity Observations] --> B[Data Collection]
    B --> C[Digitization & Standardization]
    C --> D[Quality Control]
    D --> E[Data Aggregation]
    E --> F[Data Discovery & Access]
    F --> G[Analysis & Modeling]
    G --> H[Scientific Knowledge]
    H --> I[Conservation Action]
    I --> J[Monitoring & Feedback]
    J --> A
    
    B --> K[Field Surveys]
    B --> L[Museum Specimens]
    B --> M[Citizen Science]
    B --> N[Automated Sensors]
    
    G --> O[Species Distribution Models]
    G --> P[Trend Analysis]
    G --> Q[Biodiversity Metrics]
    
    style A fill:#e1f5e1
    style H fill:#ffe1e1
    style I fill:#e1e1ff
```

This conceptual framework illustrates the biodiversity data lifecycle, from initial observations through standardization, aggregation, analysis, and ultimately to conservation applications. The feedback loop emphasizes that biodiversity monitoring is an ongoing process, with new data continuously informing understanding and action.

### Applications and Use Cases

#### Species Distribution Modeling

Species distribution models (SDMs) predict where species can occur based on environmental conditions and known occurrence locations. These models integrate biodiversity database records with environmental layers (climate, topography, land cover) to map suitable habitat and project range shifts under climate change scenarios [@elith2009].

**Mathematical foundation:**

SDMs typically relate species presence/absence or abundance to environmental predictors. A generalized linear model (GLM) framework can be expressed as:

$$
\text{logit}(p_i) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + ... + \beta_k x_{ik}
$$

where $p_i$ is the probability of species presence at location $i$, $x_{ij}$ are environmental predictors (temperature, precipitation, etc.), and $\beta_j$ are coefficients estimated from occurrence data.

More sophisticated approaches include:

- **Maximum Entropy (MaxEnt)**: Machine learning approach for presence-only data
- **Random Forests**: Ensemble methods capturing non-linear relationships
- **Boosted Regression Trees**: Combining multiple weak learners
- **Ensemble models**: Averaging predictions across multiple algorithms

#### Biodiversity Trend Analysis

Long-term biodiversity databases enable detection of population trends, range shifts, and community changes. The Living Planet Index, for example, aggregates population time-series data to quantify global vertebrate population trends [@mcrae2017].

**Statistical approaches for trend detection:**

Generalized Additive Models (GAMs) can model non-linear temporal trends:

$$
y_t = \alpha + f(t) + \varepsilon_t
$$

where $y_t$ is a biodiversity metric at time $t$, $f(t)$ is a smooth function of time, and $\varepsilon_t$ is random error.

Occupancy models explicitly account for imperfect detection:

$$
\begin{aligned}
z_i &\sim \text{Bernoulli}(\psi_i) \\
y_{ij} &\sim \text{Bernoulli}(z_i \cdot p_{ij})
\end{aligned}
$$

where $z_i$ is the true occupancy state (present/absent) at site $i$, $\psi_i$ is occupancy probability, $y_{ij}$ is the detection/non-detection at site $i$ during survey $j$, and $p_{ij}$ is detection probability.

#### Conservation Prioritization

Biodiversity databases inform systematic conservation planning by identifying:

- **Biodiversity hotspots**: Areas with high species richness or endemism
- **Threatened species locations**: Critical habitat for conservation action
- **Data gaps**: Under-sampled regions requiring survey effort
- **Protected area effectiveness**: Whether reserves capture biodiversity adequately

Algorithms like Marxan and Zonation use biodiversity data to design reserve networks that maximize conservation benefit given budget constraints.

#### Invasive Species Tracking

Early detection of invasive species is critical for rapid response. Biodiversity databases, particularly citizen science platforms, enable:

- **Range mapping**: Documenting invasion fronts
- **Spread modeling**: Predicting future invasion trajectories
- **Impact assessment**: Monitoring effects on native species
- **Management evaluation**: Assessing control effort effectiveness

#### Climate Change Research

Biodiversity databases provide evidence of biological responses to climate change:

- **Phenological shifts**: Earlier spring arrival, breeding, or flowering
- **Range shifts**: Poleward or upslope movements tracking temperature
- **Community reorganization**: Novel species assemblages
- **Extinction risk**: Species unable to track suitable climate

Historical museum specimens are particularly valuable, providing baseline data from before recent climate warming.

### Technical Infrastructure and Data Access

#### Application Programming Interfaces (APIs)

Modern biodiversity databases provide programmatic access through APIs, enabling automated data retrieval and integration into analytical workflows. Key APIs include:

**GBIF API**: RESTful API providing access to occurrence data, species information, and dataset metadata. Supports filtering by taxonomy, geography, date, and data quality criteria.

**eBird API**: Provides bird observation data with filters for location, date, and species. Requires API key for access.

**iNaturalist API**: Access to observations, species, and user data. Supports geographic and taxonomic queries.

**Taxonomic Name Resolution Services**: APIs for standardizing and validating scientific names (e.g., GBIF Species API, Catalog of Life).

#### Data Formats and Protocols

Biodiversity data are typically exchanged in formats including:

- **Darwin Core Archive (DwC-A)**: Zipped folder containing CSV files with standardized fields
- **JSON**: JavaScript Object Notation for web APIs
- **GeoJSON**: JSON format for geographic features
- **CSV/TSV**: Comma or tab-separated values for tabular data
- **XML**: Extensible Markup Language for structured data

The **Integrated Publishing Toolkit (IPT)** enables institutions to publish biodiversity data in Darwin Core format and register datasets with GBIF.

### Challenges and Future Directions

#### Taxonomic Impediment

The "taxonomic impediment" refers to the shortage of trained taxonomists and incomplete species inventories. Many species remain undescribed, particularly in hyperdiverse groups like insects and fungi. Molecular approaches (DNA barcoding, metabarcoding) offer potential to accelerate species discovery and identification, but require integration with traditional taxonomy.

#### Data Integration Across Scales

Integrating data from molecules to ecosystems remains challenging. Emerging initiatives aim to link:

- **Genetic data**: Sequences, barcodes, genomes
- **Trait data**: Functional characteristics affecting ecological roles
- **Occurrence data**: Species distributions
- **Interaction data**: Food webs, pollination networks, parasitism
- **Ecosystem data**: Productivity, nutrient cycling, energy flow

#### Real-Time Biodiversity Monitoring

Advances in sensor technology, bioacoustics, environmental DNA (eDNA), and computer vision are enabling near-real-time biodiversity monitoring. Camera traps with automated image recognition, acoustic sensors identifying species by calls, and eDNA metabarcoding detecting species from water or soil samples represent frontier technologies generating massive data streams requiring new analytical approaches.

#### Artificial Intelligence and Machine Learning

AI is transforming biodiversity data processing:

- **Automated species identification**: Computer vision models identifying organisms from photos
- **Acoustic monitoring**: Recognizing species from sound recordings
- **Data quality assessment**: Flagging suspicious records
- **Prediction and forecasting**: Anticipating biodiversity changes

However, AI models require large training datasets, may perpetuate biases, and need validation against expert knowledge.

#### Data Sovereignty and Equity

Biodiversity data often originate from biodiversity-rich developing countries but are analyzed and published by researchers in wealthy nations. Issues of data sovereignty, benefit-sharing, and equitable partnerships are increasingly recognized as essential for ethical and effective biodiversity science. The Nagoya Protocol on Access and Benefit-Sharing provides an international framework, but implementation challenges remain.

## Practical Example / Code Implementation ðŸ’»

In this section, we'll demonstrate how to access and analyze biodiversity data using Python. We'll use the GBIF API through the `pygbif` library to retrieve species occurrence data, perform basic quality filtering, and create visualizations.

### Setup and Installation

```{python}
#| code-fold: true
#| code-summary: "Show Installation Code"

# Install required libraries
%pip install -q pygbif pandas matplotlib seaborn folium geopandas contextily
```

### Importing Libraries

```{python}
#| code-fold: true
#| code-summary: "Show Import Code"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pygbif import occurrences as occ
from pygbif import species
import folium
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set visualization style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)
```

### Example 1: Retrieving Species Occurrence Data from GBIF

Let's retrieve occurrence data for the Monarch butterfly (*Danaus plexippus*), an iconic species known for its spectacular migration and conservation concerns.

```{python}
#| code-fold: true
#| code-summary: "Show Data Retrieval Code"

# First, get the species key for Monarch butterfly
species_name: str = "Danaus plexippus"
species_match = species.name_backbone(name=species_name)
species_key: int = species_match['usageKey']

print(f"Species: {species_name}")
print(f"GBIF Species Key: {species_key}")
print(f"Scientific Name: {species_match.get('scientificName', 'N/A')}")
print(f"Taxonomic Status: {species_match.get('status', 'N/A')}")
print()

# Retrieve occurrence data for North America
# Limiting to recent observations with coordinates
search_params = {
    'taxonKey': species_key,
    'hasCoordinate': True,
    'hasGeospatialIssue': False,
    'country': 'US',  # United States
    'year': '2020,2023',  # Recent years
    'limit': 1000  # Number of records to retrieve
}

print("Retrieving occurrence data from GBIF...")
results = occ.search(**search_params)

# Extract occurrence records
records = results['results']
print(f"Retrieved {len(records)} occurrence records")
```

### Example 2: Creating a DataFrame and Data Quality Filtering

```{python}
#| code-fold: true
#| code-summary: "Show Data Processing Code"

# Convert to pandas DataFrame
df = pd.DataFrame(records)

# Select relevant columns
columns_of_interest = [
    'key', 'scientificName', 'decimalLatitude', 'decimalLongitude',
    'eventDate', 'year', 'month', 'day', 'basisOfRecord',
    'coordinateUncertaintyInMeters', 'stateProvince', 'locality',
    'recordedBy', 'identifiedBy', 'occurrenceStatus'
]

# Keep only columns that exist in the data
available_columns = [col for col in columns_of_interest if col in df.columns]
df_filtered = df[available_columns].copy()

print("Data structure:")
print(df_filtered.head())
print("\nData types:")
print(df_filtered.dtypes)
print("\nBasic statistics:")
print(df_filtered.describe())
```

### Example 3: Data Quality Assessment

```{python}
#| code-fold: true
#| code-summary: "Show Quality Assessment Code"

# Assess data quality dimensions

print("=== DATA QUALITY ASSESSMENT ===\n")

# 1. Completeness
print("1. Completeness (% non-null values):")
completeness = (df_filtered.notna().sum() / len(df_filtered) * 100).round(2)
print(completeness)
print()

# 2. Coordinate precision
if 'coordinateUncertaintyInMeters' in df_filtered.columns:
    print("2. Coordinate Uncertainty:")
    uncertainty_stats = df_filtered['coordinateUncertaintyInMeters'].describe()
    print(uncertainty_stats)
    print()

# 3. Basis of record distribution
if 'basisOfRecord' in df_filtered.columns:
    print("3. Basis of Record Distribution:")
    basis_counts = df_filtered['basisOfRecord'].value_counts()
    print(basis_counts)
    print()

# 4. Temporal distribution
if 'year' in df_filtered.columns:
    print("4. Temporal Distribution:")
    year_counts = df_filtered['year'].value_counts().sort_index()
    print(year_counts)
    print()

# Apply quality filters
print("=== APPLYING QUALITY FILTERS ===\n")

# Create a copy for filtered data
df_quality = df_filtered.copy()

# Filter 1: Remove records with high coordinate uncertainty (>10km)
initial_count = len(df_quality)
if 'coordinateUncertaintyInMeters' in df_quality.columns:
    df_quality = df_quality[
        (df_quality['coordinateUncertaintyInMeters'].isna()) | 
        (df_quality['coordinateUncertaintyInMeters'] <= 10000)
    ]
    print(f"After coordinate uncertainty filter: {len(df_quality)} records ({len(df_quality)/initial_count*100:.1f}% retained)")

# Filter 2: Keep only human observations and preserved specimens
if 'basisOfRecord' in df_quality.columns:
    df_quality = df_quality[
        df_quality['basisOfRecord'].isin(['HUMAN_OBSERVATION', 'PRESERVED_SPECIMEN', 'OBSERVATION'])
    ]
    print(f"After basis of record filter: {len(df_quality)} records ({len(df_quality)/initial_count*100:.1f}% retained)")

# Filter 3: Remove records with missing coordinates
df_quality = df_quality.dropna(subset=['decimalLatitude', 'decimalLongitude'])
print(f"After coordinate completeness filter: {len(df_quality)} records ({len(df_quality)/initial_count*100:.1f}% retained)")

print(f"\nFinal dataset: {len(df_quality)} high-quality records")
```

### Example 4: Spatial Visualization

```{python}
#| code-fold: true
#| code-summary: "Show Mapping Code"

# Create an interactive map of occurrences
print("Creating interactive map...")

# Calculate map center
center_lat = df_quality['decimalLatitude'].mean()
center_lon = df_quality['decimalLongitude'].mean()

# Create base map
m = folium.Map(
    location=[center_lat, center_lon],
    zoom_start=4,
    tiles='OpenStreetMap'
)

# Add occurrence points
for idx, row in df_quality.iterrows():
    # Create popup text with information
    popup_text = f"""
    <b>Species:</b> {row.get('scientificName', 'N/A')}<br>
    <b>Date:</b> {row.get('eventDate', 'N/A')}<br>
    <b>Location:</b> {row.get('locality', 'N/A')}<br>
    <b>State:</b> {row.get('stateProvince', 'N/A')}<br>
    <b>Basis:</b> {row.get('basisOfRecord', 'N/A')}
    """
    
    folium.CircleMarker(
        location=[row['decimalLatitude'], row['decimalLongitude']],
        radius=3,
        popup=folium.Popup(popup_text, max_width=300),
        color='orange',
        fill=True,
        fillColor='orange',
        fillOpacity=0.6
    ).add_to(m)

# Add title
title_html = '''
<div style="position: fixed; 
            top: 10px; left: 50px; width: 400px; height: 90px; 
            background-color: white; border:2px solid grey; z-index:9999; 
            font-size:14px; padding: 10px">
<b>Monarch Butterfly (Danaus plexippus) Occurrences</b><br>
United States, 2020-2023<br>
Data source: GBIF
</div>
'''
m.get_root().html.add_child(folium.Element(title_html))

# Save map
m.save('monarch_occurrences_map.html')
print("Map saved as 'monarch_occurrences_map.html'")
print("Open this file in a web browser to view the interactive map")
```

### Example 5: Temporal Analysis

```{python}
#| code-fold: true
#| code-summary: "Show Temporal Analysis Code"

# Analyze temporal patterns in observations

# Monthly distribution
if 'month' in df_quality.columns:
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # Monthly counts
    monthly_counts = df_quality['month'].value_counts().sort_index()
    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
    
    axes[0].bar(monthly_counts.index, monthly_counts.values, color='steelblue')
    axes[0].set_xlabel('Month')
    axes[0].set_ylabel('Number of Observations')
    axes[0].set_title('Monarch Butterfly Observations by Month (2020-2023)')
    axes[0].set_xticks(range(1, 13))
    axes[0].set_xticklabels(month_names, rotation=45)
    axes[0].grid(axis='y', alpha=0.3)
    
    # Yearly counts
    yearly_counts = df_quality['year'].value_counts().sort_index()
    axes[1].bar(yearly_counts.index, yearly_counts.values, color='coral')
    axes[1].set_xlabel('Year')
    axes[1].set_ylabel('Number of Observations')
    axes[1].set_title('Monarch Butterfly Observations by Year')
    axes[1].grid(axis='y', alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('monarch_temporal_patterns.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("\nTemporal patterns:")
    print(f"Peak observation month: {month_names[monthly_counts.idxmax()-1]}")
    print(f"Total observations per year:\n{yearly_counts}")
```

### Example 6: Spatial Density Analysis

```{python}
#| code-fold: true
#| code-summary: "Show Spatial Analysis Code"

# Create a hexbin plot showing observation density

fig, ax = plt.subplots(figsize=(12, 8))

hexbin = ax.hexbin(
    df_quality['decimalLongitude'], 
    df_quality['decimalLatitude'],
    gridsize=30,
    cmap='YlOrRd',
    mincnt=1,
    alpha=0.8
)

ax.set_xlabel('Longitude', fontsize=12)
ax.set_ylabel('Latitude', fontsize=12)
ax.set_title('Monarch Butterfly Observation Density (2020-2023)', fontsize=14, fontweight='bold')

# Add colorbar
cbar = plt.colorbar(hexbin, ax=ax)
cbar.set_label('Number of Observations', fontsize=11)

# Add state boundaries context (simplified)
ax.set_xlim(df_quality['decimalLongitude'].min() - 1, df_quality['decimalLongitude'].max() + 1)
ax.set_ylim(df_quality['decimalLatitude'].min() - 1, df_quality['decimalLatitude'].max() + 1)

plt.tight_layout()
plt.savefig('monarch_density_map.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\nSpatial summary:")
print(f"Latitude range: {df_quality['decimalLatitude'].min():.2f}Â° to {df_quality['decimalLatitude'].max():.2f}Â°")
print(f"Longitude range: {df_quality['decimalLongitude'].min():.2f}Â° to {df_quality['decimalLongitude'].max():.2f}Â°")
```

### Example 7: Data Summary Statistics

```{python}
#| code-fold: true
#| code-summary: "Show Summary Statistics Code"

# Generate comprehensive summary statistics

print("=== COMPREHENSIVE DATA SUMMARY ===\n")

# Geographic coverage
print("Geographic Coverage:")
if 'stateProvince' in df_quality.columns:
    state_counts = df_quality['stateProvince'].value_counts().head(10)
    print("\nTop 10 states/provinces by observation count:")
    print(state_counts)

# Data contributors
print("\n\nData Contributors:")
if 'recordedBy' in df_quality.columns:
    contributor_counts = df_quality['recordedBy'].value_counts().head(5)
    print(f"Number of unique observers: {df_quality['recordedBy'].nunique()}")
    print("\nTop 5 contributors:")
    print(contributor_counts)

# Basis of record summary
print("\n\nData Types (Basis of Record):")
if 'basisOfRecord' in df_quality.columns:
    basis_pct = (df_quality['basisOfRecord'].value_counts() / len(df_quality) * 100).round(2)
    print(basis_pct)

# Create summary visualization
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. State distribution (top 10)
if 'stateProvince' in df_quality.columns:
    state_counts.plot(kind='barh', ax=axes[0, 0], color='steelblue')
    axes[0, 0].set_xlabel('Number of Observations')
    axes[0, 0].set_title('Top 10 States by Observation Count')
    axes[0, 0].invert_yaxis()

# 2. Basis of record pie chart
if 'basisOfRecord' in df_quality.columns:
    basis_counts = df_quality['basisOfRecord'].value_counts()
    axes[0, 1].pie(basis_counts.values, labels=basis_counts.index, autopct='%1.1f%%', startangle=90)
    axes[0, 1].set_title('Observation Types')

# 3. Monthly pattern
if 'month' in df_quality.columns:
    monthly_counts = df_quality['month'].value_counts().sort_index()
    axes[1, 0].plot(monthly_counts.index, monthly_counts.values, marker='o', linewidth=2, color='coral')
    axes[1, 0].set_xlabel('Month')
    axes[1, 0].set_ylabel('Number of Observations')
    axes[1, 0].set_title('Seasonal Pattern')
    axes[1, 0].set_xticks(range(1, 13))
    axes[1, 0].grid(alpha=0.3)

# 4. Coordinate uncertainty distribution
if 'coordinateUncertaintyInMeters' in df_quality.columns:
    uncertainty_data = df_quality['coordinateUncertaintyInMeters'].dropna()
    if len(uncertainty_data) > 0:
        axes[1, 1].hist(uncertainty_data, bins=30, color='green', alpha=0.7, edgecolor='black')
        axes[1, 1].set_xlabel('Coordinate Uncertainty (meters)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Spatial Precision Distribution')
        axes[1, 1].set_xlim(0, uncertainty_data.quantile(0.95))

plt.tight_layout()
plt.savefig('monarch_data_summary.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n=== Analysis complete! ===")
```

### Key Takeaways from the Code Examples

This practical demonstration illustrated several important concepts:

1. **API Access**: Using `pygbif` to programmatically retrieve biodiversity data from GBIF
2. **Data Quality Filtering**: Applying multiple quality criteria to ensure reliable data
3. **Spatial Analysis**: Mapping occurrence points and analyzing geographic patterns
4. **Temporal Analysis**: Examining seasonal and annual patterns in observations
5. **Data Exploration**: Generating summary statistics and visualizations

These skills form the foundation for working with biodiversity databases in environmental science research and applications.

## Student Exercise ðŸ“

### Exercise: Analyzing Biodiversity Patterns for a Species of Your Choice

**Objective**: Apply the concepts and techniques learned to analyze biodiversity data for a species relevant to your region or interests.

**Instructions**:

1. **Species Selection** (5 minutes):
   - Choose a species of interest (bird, mammal, plant, insect, etc.)
   - Verify that the species has sufficient occurrence data in GBIF (at least 500 records)
   - Document your species choice with scientific name and common name

2. **Data Retrieval** (10 minutes):
   - Use the `pygbif` library to retrieve occurrence data for your chosen species
   - Focus on a specific geographic region (country or continent)
   - Limit data to recent years (e.g., 2015-2023)
   - Retrieve at least 500-1000 records

3. **Data Quality Assessment** (15 minutes):
   - Calculate completeness metrics for key fields (coordinates, dates, identification)
   - Assess coordinate uncertainty distribution
   - Examine basis of record types
   - Apply appropriate quality filters:
     - Remove records with coordinate uncertainty > 5000 meters
     - Keep only records with coordinates
     - Filter to reliable basis of record types
   - Document how many records remain after filtering

4. **Spatial Analysis** (15 minutes):
   - Create an interactive map showing occurrence locations
   - Generate a density plot (hexbin or heatmap) showing observation hotspots
   - Identify the top 5 locations (states/provinces/regions) with most observations
   - Calculate the geographic range (latitude and longitude extents)

5. **Temporal Analysis** (10 minutes):
   - Analyze monthly observation patterns
   - Identify seasonal peaks (if any)
   - Compare observation counts across years
   - Discuss whether temporal patterns reflect biological phenomena (migration, breeding) or sampling bias

6. **Synthesis and Interpretation** (5 minutes):
   - Write a brief summary (200-300 words) interpreting your findings:
     - What do the spatial patterns reveal about the species' distribution?
     - Are there data gaps or biases evident in the dataset?
     - What ecological or conservation insights can be drawn?
     - What are the limitations of the data for answering specific research questions?

**Deliverables**:

- Documented Python code with comments explaining each step
- At least 3 visualizations (map, temporal plot, spatial density)
- Written summary addressing the synthesis questions
- Table of data quality metrics before and after filtering

**Bonus Challenges** (Optional):

- Compare your species' distribution to a closely related species
- Analyze data from two different time periods to detect range changes
- Integrate environmental data (climate, land cover) with occurrence data
- Identify potential data quality issues specific to your species

**Resources**:

- GBIF Species Portal: https://www.gbif.org/species/search
- `pygbif` documentation: https://pygbif.readthedocs.io/
- Darwin Core terms: https://dwc.tdwg.org/terms/

## Exercise Solution ðŸ”‘

Here's a sample solution using the American Robin (*Turdus migratorius*) as an example:

```{python}
#| code-fold: true
#| code-summary: "Show Complete Solution Code"

# SOLUTION: American Robin Biodiversity Analysis

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pygbif import occurrences as occ
from pygbif import species
import folium
import warnings
warnings.filterwarnings('ignore')

print("=== BIODIVERSITY DATA ANALYSIS ===")
print("Species: American Robin (Turdus migratorius)\n")

# Step 1: Species Selection and Data Retrieval
species_name = "Turdus migratorius"
species_match = species.name_backbone(name=species_name)
species_key = species_match['usageKey']

print(f"Scientific Name: {species_match['scientificName']}")
print(f"Common Name: American Robin")
print(f"GBIF Species Key: {species_key}\n")

# Retrieve data
search_params = {
    'taxonKey': species_key,
    'hasCoordinate': True,
    'hasGeospatialIssue': False,
    'country': 'US',
    'year': '2015,2023',
    'limit': 1000
}

print("Retrieving occurrence data...")
results = occ.search(**search_params)
records = results['results']
df = pd.DataFrame(records)

print(f"Initial records retrieved: {len(df)}\n")

# Step 2: Data Quality Assessment
print("=== DATA QUALITY ASSESSMENT ===\n")

# Select relevant columns
columns_of_interest = [
    'key', 'scientificName', 'decimalLatitude', 'decimalLongitude',
    'eventDate', 'year', 'month', 'basisOfRecord',
    'coordinateUncertaintyInMeters', 'stateProvince', 'locality'
]
available_columns = [col for col in columns_of_interest if col in df.columns]
df_filtered = df[available_columns].copy()

# Completeness metrics
print("Completeness (% non-null):")
completeness = (df_filtered.notna().sum() / len(df_filtered) * 100).round(2)
print(completeness.to_string())
print()

# Coordinate uncertainty
if 'coordinateUncertaintyInMeters' in df_filtered.columns:
    print("Coordinate Uncertainty Statistics:")
    print(df_filtered['coordinateUncertaintyInMeters'].describe())
    print()

# Basis of record
if 'basisOfRecord' in df_filtered.columns:
    print("Basis of Record Distribution:")
    print(df_filtered['basisOfRecord'].value_counts())
    print()

# Apply quality filters
print("=== APPLYING QUALITY FILTERS ===\n")
initial_count = len(df_filtered)

# Filter 1: Coordinate uncertainty
if 'coordinateUncertaintyInMeters' in df_filtered.columns:
    df_quality = df_filtered[
        (df_filtered['coordinateUncertaintyInMeters'].isna()) | 
        (df_filtered['coordinateUncertaintyInMeters'] <= 5000)
    ]
else:
    df_quality = df_filtered.copy()

print(f"After uncertainty filter: {len(df_quality)} records")

# Filter 2: Basis of record
if 'basisOfRecord' in df_quality.columns:
    df_quality = df_quality[
        df_quality['basisOfRecord'].isin(['HUMAN_OBSERVATION', 'PRESERVED_SPECIMEN', 'OBSERVATION'])
    ]
print(f"After basis filter: {len(df_quality)} records")

# Filter 3: Complete coordinates
df_quality = df_quality.dropna(subset=['decimalLatitude', 'decimalLongitude'])
print(f"After coordinate filter: {len(df_quality)} records")
print(f"Retention rate: {len(df_quality)/initial_count*100:.1f}%\n")

# Step 3: Spatial Analysis
print("=== SPATIAL ANALYSIS ===\n")

# Top locations
if 'stateProvince' in df_quality.columns:
    top_locations = df_quality['stateProvince'].value_counts().head(5)
    print("Top 5 locations by observation count:")
    print(top_locations)
    print()

# Geographic range
lat_range = (df_quality['decimalLatitude'].min(), df_quality['decimalLatitude'].max())
lon_range = (df_quality['decimalLongitude'].min(), df_quality['decimalLongitude'].max())
print(f"Latitude range: {lat_range[0]:.2f}Â° to {lat_range[1]:.2f}Â°")
print(f"Longitude range: {lon_range[0]:.2f}Â° to {lon_range[1]:.2f}Â°")
print(f"Geographic extent: {lat_range[1] - lat_range[0]:.2f}Â° latitude Ã— {lon_range[1] - lon_range[0]:.2f}Â° longitude\n")

# Create visualizations
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Spatial density hexbin
hexbin = axes[0, 0].hexbin(
    df_quality['decimalLongitude'], 
    df_quality['decimalLatitude'],
    gridsize=25,
    cmap='YlOrRd',
    mincnt=1
)
axes[0, 0].set_xlabel('Longitude')
axes[0, 0].set_ylabel('Latitude')
axes[0, 0].set_title('Observation Density')
plt.colorbar(hexbin, ax=axes[0, 0], label='Count')

# 2. Monthly pattern
if 'month' in df_quality.columns:
    monthly_counts = df_quality['month'].value_counts().sort_index()
    axes[0, 1].bar(monthly_counts.index, monthly_counts.values, color='steelblue')
    axes[0, 1].set_xlabel('Month')
    axes[0, 1].set_ylabel('Observations')
    axes[0, 1].set_title('Seasonal Pattern')
    axes[0, 1].set_xticks(range(1, 13))

# 3. Annual trend
if 'year' in df_quality.columns:
    yearly_counts = df_quality['year'].value_counts().sort_index()
    axes[1, 0].plot(yearly_counts.index, yearly_counts.values, marker='o', linewidth=2, color='coral')
    axes[1, 0].set_xlabel('Year')
    axes[1, 0].set_ylabel('Observations')
    axes[1, 0].set_title('Annual Trend (2015-2023)')
    axes[1, 0].grid(alpha=0.3)

# 4. Top locations
if 'stateProvince' in df_quality.columns:
    top_locations.plot(kind='barh', ax=axes[1, 1], color='green')
    axes[1, 1].set_xlabel('Number of Observations')
    axes[1, 1].set_title('Top 5 States')
    axes[1, 1].invert_yaxis()

plt.tight_layout()
plt.savefig('robin_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# Step 4: Temporal Analysis
print("=== TEMPORAL ANALYSIS ===\n")

if 'month' in df_quality.columns:
    monthly_counts = df_quality['month'].value_counts().sort_index()
    peak_month = monthly_counts.idxmax()
    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 
                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
    print(f"Peak observation month: {month_names[peak_month-1]}")
    print(f"Observations range from {monthly_counts.min()} to {monthly_counts.max()} per month\n")

if 'year' in df_quality.columns:
    yearly_counts = df_quality['year'].value_counts().sort_index()
    print("Annual observation counts:")
    print(yearly_counts)
    print()

# Step 5: Synthesis
print("=== SYNTHESIS AND INTERPRETATION ===\n")
print("""
The American Robin occurrence data reveals several important patterns:

SPATIAL DISTRIBUTION: The species shows widespread distribution across the United
States, with highest observation density in northeastern and midwestern states.
This aligns with the robin's known range as a common breeding bird throughout
much of North America. The data shows clear geographic clustering near urban and
suburban areas, likely reflecting both the species' habitat preferences and
observer effort bias toward populated regions.

TEMPORAL PATTERNS: Monthly observation patterns show peaks during spring and
summer months (April-July), corresponding to the breeding season when robins are
most conspicuous and active. The decline in winter observations may reflect both
southward migration and reduced observer effort during colder months. Annual
trends show relatively stable observation numbers, with slight increases in
recent years potentially reflecting growing participation in citizen science
platforms like eBird.

DATA QUALITY AND BIASES: After quality filtering, approximately 90% of records
were retained, indicating generally high data quality. However, several biases
are evident: (1) Geographic bias toward accessible areas near roads and cities;
(2) Temporal bias toward breeding season; (3) Taxonomic bias toward a common,
easily identified species. These patterns underscore the importance of
understanding sampling effort when interpreting biodiversity data.

CONSERVATION INSIGHTS: The robust dataset for American Robins provides a valuable
baseline for monitoring population trends and detecting range shifts. However,
the data are most useful for broad-scale patterns rather than fine-scale habitat
associations due to spatial uncertainty and sampling biases.

LIMITATIONS: The dataset cannot reliably estimate absolute abundance, detect
rare or localized populations, or fully characterize winter distributions.
Integration with systematic survey data (e.g., Breeding Bird Survey) would
strengthen inference about population trends.
""")

print("=== ANALYSIS COMPLETE ===")
```

### Key Points from Solution:

1. **Systematic Approach**: The solution follows a structured workflow from data retrieval through quality control to analysis and interpretation

2. **Quality Filtering**: Multiple filters applied sequentially, with documentation of retention rates

3. **Multiple Analyses**: Spatial, temporal, and data quality dimensions examined

4. **Critical Interpretation**: The synthesis acknowledges both insights and limitations, recognizing sampling biases

5. **Reproducibility**: Well-commented code that others could adapt for different species

## Quiz ðŸ“‹

Test your understanding of biodiversity databases and ecological data:

### Question 1

What is the primary purpose of the Global Biodiversity Information Facility (GBIF)?

A) To conduct field surveys of endangered species  
B) To aggregate and provide open access to biodiversity occurrence data from multiple sources  
C) To classify organisms using DNA barcoding  
D) To manage protected areas worldwide

::: {.callout-note collapse="true"}
## Answer

**B) To aggregate and provide open access to biodiversity occurrence data from multiple sources**

GBIF is a data aggregator that provides a unified portal for accessing occurrence records from thousands of institutions worldwide, operating on principles of open access and standardization.
:::

### Question 2

Which standardized data format is most widely used for sharing biodiversity occurrence data?

A) GeoJSON  
B) Darwin Core  
C) NetCDF  
D) HDF5

::: {.callout-note collapse="true"}
## Answer

**B) Darwin Core**

Darwin Core is the most widely adopted standard for biodiversity data, defining a vocabulary of terms for describing species occurrences and related information. It was developed by the Biodiversity Information Standards (TDWG) organization.
:::

### Question 3

True or False: Citizen science platforms like eBird and iNaturalist can produce scientifically valuable biodiversity data despite varying observer expertise.

::: {.callout-note collapse="true"}
## Answer

**True**

Citizen science platforms generate massive datasets that would be impossible through traditional surveys alone. While data quality varies, platforms implement quality control mechanisms (expert review, automated filters, statistical models) that make the data scientifically valuable. Research has demonstrated that citizen science data can reliably detect biodiversity patterns and trends.
:::

### Question 4

What does the Darwin Core term "coordinateUncertaintyInMeters" represent?

A) The distance between observation points  
B) The radius of uncertainty around the recorded coordinates  
C) The elevation at which the observation occurred  
D) The distance traveled by the observer

::: {.callout-note collapse="true"}
## Answer

**B) The radius of uncertainty around the recorded coordinates**

This field quantifies spatial precision, indicating the radius within which the true location is expected to fall. High values indicate low spatial precision, which is important for filtering data and interpreting spatial patterns.
:::

### Question 5

Which of the following represents a major source of bias in biodiversity occurrence databases?

A) Geographic bias toward accessible areas near roads and cities  
B) Taxonomic bias toward charismatic or easily identified species  
C) Temporal bias reflecting seasonal observer effort  
D) All of the above

::: {.callout-note collapse="true"}
## Answer

**D) All of the above**

Biodiversity occurrence data are subject to multiple, interacting biases. Geographic bias results from observers sampling accessible areas; taxonomic bias reflects greater interest in and ability to identify certain groups; temporal bias arises from seasonal variation in both species activity and observer effort. Understanding these biases is essential for appropriate data analysis and interpretation.
:::

### Question 6

What is the primary advantage of museum specimen records compared to observational data?

A) They are always more spatially precise  
B) They provide physical vouchers that can be re-examined and verified  
C) They cover more recent time periods  
D) They include more behavioral information

::: {.callout-note collapse="true"}
## Answer

**B) They provide physical vouchers that can be re-examined and verified**

Museum specimens serve as permanent physical evidence that can be re-examined as taxonomic understanding evolves or new analytical techniques (e.g., genetic analysis) become available. This verifiability is a key advantage over observational records that cannot be independently confirmed.
:::

### Question 7

True or False: Species distribution models (SDMs) can accurately predict species presence using only occurrence data without any environmental variables.

::: {.callout-note collapse="true"}
## Answer

**False**

SDMs relate species occurrences to environmental conditions (climate, topography, land cover, etc.) to predict suitable habitat. Occurrence data alone cannot predict where species can occur in unsampled areasâ€”environmental predictors are essential for modeling the ecological niche and projecting distributions.
:::

### Question 8

What does "basis of record" indicate in a biodiversity occurrence record?

A) The funding source for the observation  
B) The type of evidence supporting the occurrence (e.g., specimen, observation, genetic)  
C) The taxonomic authority used for identification  
D) The database where the record is stored

::: {.callout-note collapse="true"}
## Answer

**B) The type of evidence supporting the occurrence (e.g., specimen, observation, genetic)**

The "basisOfRecord" field in Darwin Core indicates the nature of the evidence: preserved specimen, human observation, machine observation, living specimen, fossil specimen, etc. This information is important for assessing record reliability and determining appropriate uses of the data.
:::

### Question 9

Which of the following is NOT a typical application of biodiversity occurrence databases?

A) Tracking species range shifts associated with climate change  
B) Identifying biodiversity hotspots for conservation prioritization  
C) Predicting tomorrow's weather  
D) Detecting invasive species spread

::: {.callout-note collapse="true"}
## Answer

**C) Predicting tomorrow's weather**

Biodiversity databases document species distributions and are used for ecological and conservation applications, not meteorological forecasting. Weather prediction relies on atmospheric data from weather stations, satellites, and climate models, not biological occurrence records.
:::

### Question 10

What is the main purpose of Ecological Metadata Language (EML)?

A) To encrypt sensitive species location data  
B) To translate species names between languages  
C) To document dataset characteristics, collection methods, and structure for long-term interpretability  
D) To compress large biodiversity datasets

::: {.callout-note collapse="true"}
## Answer

**C) To document dataset characteristics, collection methods, and structure for long-term interpretability**

EML provides a standardized format for documenting ecological datasets, ensuring that data remain interpretable and usable long after collection. It describes dataset identity, structure, collection methods, geographic and temporal coverage, and other metadata essential for data reuse.
:::

## References

::: {#refs}
:::