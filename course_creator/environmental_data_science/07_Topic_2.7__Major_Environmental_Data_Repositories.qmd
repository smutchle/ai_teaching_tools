---
title: "Major Environmental Data Repositories"
subtitle: "Lecture 2: Data Types and Sources in Environmental Science"
bibliography: major_environmental_data_repositories.bib
---

## Topic Overview ðŸŒ

Environmental science in the 21st century is fundamentally data-driven. The ability to access, understand, and utilize vast repositories of environmental data has transformed how we monitor our planet, understand environmental changes, and make informed decisions about conservation and resource management. Major environmental data repositories serve as the backbone of modern environmental research, providing scientists, policymakers, and the public with access to decades of observations about Earth's atmosphere, oceans, land surfaces, and ecosystems.

This topic explores the landscape of major environmental data repositories, with a focus on four key institutional sources: NASA's Earth Observing System Data and Information System (EOSDIS), the National Oceanic and Atmospheric Administration (NOAA) databases, the United States Geological Survey (USGS) Earth Explorer, and the European Space Agency (ESA) archives. Each of these repositories represents decades of investment in Earth observation infrastructure and provides unique datasets that are essential for understanding environmental processes at local, regional, and global scales.

Understanding these repositories is not merely about knowing where data existsâ€”it involves comprehending the types of data available, the policies governing access, the technical interfaces for data retrieval, and the quality and limitations of different datasets. This knowledge is foundational for any environmental scientist, as it enables efficient data discovery, appropriate dataset selection, and effective integration of multiple data sources into comprehensive environmental analyses.

**Relevance to Environmental Science:**

The importance of major environmental data repositories cannot be overstated in the context of modern environmental science. These repositories enable:

- **Long-term environmental monitoring**: Access to decades of consistent observations allows scientists to detect trends, identify anomalies, and understand the pace and direction of environmental change.
- **Global-scale analysis**: Satellite-based observations provide unprecedented spatial coverage, enabling studies that would be impossible with ground-based measurements alone.
- **Interdisciplinary research**: By housing diverse data typesâ€”from atmospheric chemistry to land cover to ocean temperatureâ€”these repositories facilitate integrated environmental studies.
- **Reproducible science**: Open access to standardized, well-documented datasets ensures that research findings can be verified and built upon by the broader scientific community.
- **Informed decision-making**: Policymakers and resource managers rely on these data sources to monitor environmental conditions, assess risks, and evaluate the effectiveness of interventions.

As we face pressing environmental challenges including climate change, biodiversity loss, and resource depletion, the role of these data repositories becomes increasingly critical. They provide the empirical foundation for understanding our planet's systems and tracking our progress toward environmental sustainability goals.

## Background & Theory ðŸ“š

### Historical Context: The Evolution of Environmental Data Infrastructure

The development of major environmental data repositories reflects the evolution of Earth observation capabilities and the growing recognition of environmental data as a public good. Prior to the satellite era, environmental data collection was primarily limited to ground-based weather stations, ocean buoys, and field surveys. While valuable, these observations provided only sparse spatial coverage and were often difficult to access or compare across different collection agencies.

The launch of the first weather satellites in the 1960s marked a transformative moment in environmental monitoring. The Television Infrared Observation Satellite (TIROS-1), launched by NASA in 1960, demonstrated the potential for space-based Earth observation [@kidder2005satellite]. This was followed by increasingly sophisticated satellite missions that expanded beyond meteorology to include land surface monitoring, ocean observations, and atmospheric composition measurements.

As satellite missions proliferated and data volumes grew exponentially, the need for organized data management systems became apparent. In the 1990s, NASA established the Earth Observing System (EOS) as part of its Mission to Planet Earth program, creating a systematic approach to Earth observation that integrated multiple satellites with a comprehensive data management infrastructure [@asrar2001earth]. This represented a paradigm shift from mission-specific data archives to an integrated, interdisciplinary data system designed to support long-term environmental research.

Similarly, NOAA evolved from its origins in weather forecasting to become a comprehensive environmental data provider, integrating atmospheric, oceanic, and terrestrial observations. The USGS expanded its traditional focus on geological surveys to include extensive Earth observation capabilities through the Landsat programâ€”the longest continuous Earth observation program in history, beginning in 1972 [@wulder2019current]. The European Space Agency developed its own Earth observation capabilities through programs like Copernicus, which provides free and open access to Sentinel satellite data.

### NASA Earth Observing System Data and Information System (EOSDIS)

**System Architecture and Organization:**

NASA's EOSDIS represents one of the most comprehensive environmental data systems in the world, managing petabytes of Earth observation data from multiple satellite missions. The system is organized around several Distributed Active Archive Centers (DAACs), each specializing in particular domains of Earth science:

- **Atmospheric DAACs**: Focus on atmospheric composition, dynamics, and radiation
- **Land DAACs**: Specialize in land surface properties, vegetation, and cryosphere
- **Ocean DAACs**: Manage ocean color, sea surface temperature, and ocean circulation data
- **Interdisciplinary DAACs**: Handle cross-cutting datasets and provide specialized processing services

This distributed architecture reflects the recognition that different scientific communities have unique expertise and requirements. Each DAAC develops specialized tools, processing algorithms, and user services tailored to its scientific domain while maintaining interoperability with the broader EOSDIS infrastructure.

**Key Satellite Missions and Data Products:**

EOSDIS provides access to data from numerous NASA Earth observation missions, including:

1. **Terra and Aqua**: Flagship EOS satellites carrying multiple instruments for comprehensive Earth system observations. The Moderate Resolution Imaging Spectroradiometer (MODIS) aboard these satellites provides daily global coverage for land, ocean, and atmospheric studies [@justice2002moderate].

2. **Landsat Series**: Joint NASA/USGS program providing moderate-resolution multispectral imagery since 1972. Landsat data is fundamental for land cover change detection and resource management.

3. **GRACE and GRACE-FO**: Gravity Recovery and Climate Experiment missions measure Earth's gravity field variations, enabling monitoring of groundwater depletion, ice mass changes, and sea level rise [@tapley2019contributions].

4. **ICESat-2**: Advanced laser altimetry mission measuring ice sheet elevation changes, forest canopy height, and other surface topography with unprecedented precision.

5. **SMAP**: Soil Moisture Active Passive mission monitors soil moisture and freeze-thaw states, critical for agricultural and hydrological applications.

**Data Access Policies and Mechanisms:**

NASA maintains an open data policy, making EOSDIS data freely available to all users without restrictions. This policy, formalized in NASA's Earth Science Data and Information Policy, reflects the principle that environmental data collected with public funds should be publicly accessible [@nasa2014data]. The policy includes:

- No registration requirements for data access (though registration enables additional services)
- No cost for data products
- No restrictions on data use or redistribution
- Commitment to long-term data preservation and accessibility

Access mechanisms include:

- **Earthdata Search**: Web-based interface for discovering and downloading data across all DAACs
- **Application Programming Interfaces (APIs)**: Programmatic access for automated data retrieval
- **Direct Access**: Subsetting and customization services that allow users to request specific spatial regions, time periods, or variables
- **Cloud-based Access**: Increasing availability of data in cloud environments (AWS, Google Cloud) to enable in-place analysis without downloading

### NOAA National Centers for Environmental Information (NCEI)

**Organizational Structure and Data Holdings:**

NOAA's NCEI, formed in 2015 by merging three data centers, manages one of the world's largest archives of atmospheric, coastal, oceanic, and geophysical data. The organization preserves and provides access to over 37 petabytes of environmental data, with holdings growing by approximately 5 terabytes daily [@ncei2021about].

**Major Data Categories:**

1. **Climate Data**: Historical weather observations, climate normals, and long-term climate records essential for understanding climate variability and change. This includes:
   - Global Historical Climatology Network (GHCN): Quality-controlled temperature and precipitation records from thousands of stations worldwide
   - Climate Data Online (CDO): Provides access to over 200 years of climate data
   - Paleoclimatology data: Proxy records from tree rings, ice cores, and sediments extending Earth's climate record thousands to millions of years into the past

2. **Oceanographic Data**: Comprehensive ocean observations including:
   - World Ocean Database: Temperature, salinity, and other oceanographic profiles
   - Sea surface temperature products from satellites and in-situ measurements
   - Ocean acidification data from monitoring networks
   - Coastal and marine ecosystem observations

3. **Atmospheric Data**: Including satellite observations, radar data, and atmospheric composition measurements critical for weather forecasting and air quality monitoring.

4. **Geophysical Data**: Solid Earth observations including seismic data, geomagnetic measurements, and gravity field information.

**Data Access and User Services:**

NOAA provides multiple pathways for data access:

- **Web-based data portals**: User-friendly interfaces for browsing and downloading data
- **FTP servers**: Direct file access for bulk downloads
- **THREDDS Data Servers**: Standards-based access to gridded datasets
- **Web services and APIs**: RESTful APIs and OPeNDAP services for programmatic access
- **Custom data requests**: Services for specialized data extractions and quality control

NOAA's data policy emphasizes free and open access, though some operational products may have restrictions during their initial release period. The organization also maintains rigorous data quality standards and provides extensive documentation about data collection methods, processing procedures, and known limitations.

### USGS Earth Explorer and Land Resources

**Earth Explorer Platform:**

The USGS Earth Explorer serves as the primary interface for accessing the vast archives of land remote sensing data managed by the USGS. The platform provides access to over 50 years of Earth observation imagery, including:

**Landsat Archive:**

Landsat represents the longest continuous space-based record of Earth's land surface. The program's longevity and consistent observation strategy make it invaluable for change detection studies [@woodcock2008free]. Key characteristics include:

- **Temporal Coverage**: 1972 to present, with ongoing missions ensuring continuity
- **Spatial Resolution**: 30 meters for multispectral bands (15m for panchromatic)
- **Spectral Bands**: Visible, near-infrared, shortwave infrared, and thermal infrared
- **Revisit Time**: 16 days (8 days when combining Landsat 8 and 9)
- **Global Coverage**: Systematic acquisition of all land areas

The USGS processes Landsat data to various levels, including:

- **Level-1**: Radiometrically calibrated and geometrically corrected
- **Level-2**: Surface reflectance products with atmospheric correction applied
- **Analysis Ready Data (ARD)**: Tiled, consistently processed data optimized for time-series analysis

**Additional Data Holdings:**

Beyond Landsat, Earth Explorer provides access to:

- **Sentinel-2**: European Space Agency's high-resolution multispectral imagery (10-20m resolution)
- **MODIS**: Daily global coverage at moderate resolution (250-1000m)
- **Digital Elevation Models**: Including SRTM, ASTER GDEM, and lidar-derived topography
- **Aerial Photography**: Historical aerial imagery providing pre-satellite era land surface records
- **Hyperspectral Data**: From missions like Hyperion and AVIRIS

**Data Processing and Quality:**

USGS applies sophisticated processing chains to ensure data quality:

1. **Geometric Correction**: Precise geolocation using ground control points and digital elevation models
2. **Radiometric Calibration**: Conversion from digital numbers to physical units (radiance, reflectance)
3. **Atmospheric Correction**: Removal of atmospheric effects to retrieve surface properties
4. **Cloud Masking**: Automated identification of clouds and cloud shadows
5. **Quality Assessment**: Per-pixel quality flags indicating data reliability

### European Space Agency (ESA) Archives

**Copernicus Programme:**

The European Union's Copernicus programme represents Europe's commitment to operational Earth observation. The programme operates a constellation of Sentinel satellites, each designed for specific observation requirements:

**Sentinel Mission Overview:**

```{mermaid}
graph TD
    A[Copernicus Programme] --> B[Sentinel-1: SAR Imaging]
    A --> C[Sentinel-2: Optical Imaging]
    A --> D[Sentinel-3: Ocean & Land Monitoring]
    A --> E[Sentinel-5P: Atmospheric Monitoring]
    A --> F[Sentinel-6: Ocean Topography]
    
    B --> B1[All-weather imaging]
    B --> B2[Surface deformation]
    B --> B3[Flood mapping]
    
    C --> C1[Vegetation monitoring]
    C --> C2[Land cover mapping]
    C --> C3[Water quality]
    
    D --> D1[Sea surface temperature]
    D --> D2[Ocean color]
    D --> D3[Land surface temperature]
    
    E --> E1[Air quality]
    E --> E2[Ozone monitoring]
    E --> E3[Greenhouse gases]
    
    F --> F1[Sea level rise]
    F --> F2[Ocean circulation]
```

**Sentinel-1 (Synthetic Aperture Radar):**

Sentinel-1 provides all-weather, day-and-night radar imagery crucial for:

- Monitoring sea ice extent and ice sheet dynamics
- Detecting ground deformation from earthquakes, volcanoes, and subsidence
- Mapping floods and monitoring agricultural conditions
- Maritime surveillance and ship detection

The C-band SAR operates in multiple modes with spatial resolutions from 5 to 40 meters and provides systematic global coverage with a 6-12 day revisit time (using both Sentinel-1A and 1B).

**Sentinel-2 (Multispectral Optical Imaging):**

Sentinel-2 provides high-resolution optical imagery optimized for land monitoring:

- **13 spectral bands**: From visible to shortwave infrared
- **Spatial resolution**: 10m (visible and NIR), 20m (red edge and SWIR), 60m (atmospheric correction bands)
- **Swath width**: 290 km enabling wide area coverage
- **Revisit time**: 5 days at the equator (with both satellites)

The mission's design specifically supports vegetation monitoring, land cover classification, and water quality assessment with spectral bands positioned to capture key vegetation properties.

**Data Access Infrastructure:**

ESA provides multiple access pathways:

- **Copernicus Open Access Hub**: Web interface for searching and downloading Sentinel data
- **Copernicus Data Space Ecosystem**: Cloud-based platform providing data access and processing capabilities
- **APIs**: RESTful APIs for programmatic data discovery and retrieval
- **Collaborative Platforms**: Integration with third-party platforms like Google Earth Engine

ESA maintains a full, free, and open data policy for Copernicus data, ensuring that all users can access the complete archive without restrictions. Data is typically available within hours of acquisition, supporting near-real-time applications.

### Data Access Policies: Comparative Analysis

Understanding data access policies is crucial for effective use of environmental data repositories. While all major repositories have moved toward open access, important differences exist:

**Open vs. Restricted Access:**

Most governmental environmental data repositories now follow open data principles, but implementation varies:

- **NASA/USGS**: Completely open, no registration required for basic access
- **NOAA**: Generally open, though some operational products may have initial distribution restrictions
- **ESA**: Full and open access to Copernicus data; some older missions may have restrictions
- **Commercial Providers**: Often require licensing agreements and fees, though some offer free access for research or specific use cases

**Registration Requirements:**

While data may be open, repositories often implement registration systems:

- **Benefits of Registration**: Enables personalized services, bulk download capabilities, API access, and notification of new data availability
- **Privacy Considerations**: Users should review data use policies regarding how registration information is utilized
- **International Access**: Most repositories do not restrict access based on nationality, though some countries may have export control restrictions on very high-resolution data

**Data Use Restrictions:**

Even open data may come with usage guidelines:

- **Attribution Requirements**: Most repositories request citation of data sources in publications
- **Redistribution**: Generally permitted, though users should check specific terms
- **Commercial Use**: Typically allowed for government data, but users should verify
- **Liability**: Data providers generally disclaim warranties and liability for data use

**Sustainability and Long-term Access:**

Data repositories must ensure long-term preservation and accessibility:

- **Archival Standards**: Use of standardized formats and metadata ensures long-term readability
- **Data Migration**: Periodic migration to new storage technologies prevents data loss
- **Version Control**: Maintaining multiple processing versions allows users to understand how products evolve
- **Funding Stability**: Government commitment to long-term funding ensures continued data availability

### User Interfaces and Data Discovery

**Web-Based Portals:**

Modern data repositories provide sophisticated web interfaces that balance ease of use with powerful search capabilities:

**Search Capabilities:**

- **Spatial Search**: Interactive maps allowing users to define areas of interest through bounding boxes, polygons, or point locations
- **Temporal Search**: Date range selection with calendar interfaces
- **Metadata Search**: Filtering by sensor type, processing level, cloud cover, quality flags, and other attributes
- **Preview Capabilities**: Browse imagery and quicklook images before downloading full datasets

**Visualization Tools:**

Many repositories now include built-in visualization capabilities:

- **Interactive viewers**: Pan, zoom, and layer comparison tools
- **Time-series animation**: Visualizing temporal changes
- **Band combination tools**: Creating custom false-color composites
- **Analysis functions**: Basic calculations like vegetation indices

**Programmatic Access:**

For researchers working with large datasets or requiring automated workflows, programmatic access is essential:

**Application Programming Interfaces (APIs):**

APIs provide structured methods for software to interact with data repositories:

- **RESTful APIs**: HTTP-based interfaces using standard web protocols
- **OPeNDAP**: Protocol for accessing gridded scientific data over the internet
- **OGC Web Services**: Standardized geospatial web services (WMS, WCS, WFS)
- **STAC (SpatioTemporal Asset Catalog)**: Emerging standard for describing geospatial data

**Benefits of Programmatic Access:**

- **Automation**: Scripts can search, filter, and download data without manual intervention
- **Reproducibility**: Code-based workflows can be shared and repeated
- **Efficiency**: Batch processing of multiple datasets
- **Integration**: Combining data from multiple sources in unified workflows

### Types of Environmental Datasets Available

Environmental data repositories host diverse data types, each serving different research needs:

**Raster Data:**

Grid-based datasets where each cell contains a value:

- **Satellite imagery**: Multispectral and hyperspectral images
- **Climate grids**: Temperature, precipitation, and other meteorological variables
- **Derived products**: Vegetation indices, land surface temperature, snow cover

**Vector Data:**

Discrete features represented as points, lines, or polygons:

- **Station observations**: Weather stations, water quality monitoring sites
- **Administrative boundaries**: Political boundaries, protected areas
- **Transportation networks**: Roads, railways, waterways

**Time-Series Data:**

Observations at regular or irregular time intervals:

- **Climate records**: Long-term temperature and precipitation series
- **Sensor networks**: Continuous monitoring data from IoT devices
- **Satellite time-series**: Regular observations of the same location over time

**Model Outputs:**

Results from numerical simulations:

- **Weather forecasts**: Predictions of atmospheric conditions
- **Climate projections**: Long-term climate scenarios
- **Reanalysis products**: Optimal blending of observations and models to create consistent historical records

### Data Quality and Documentation

**Quality Indicators:**

Understanding data quality is essential for appropriate use:

- **Accuracy**: How close measurements are to true values
- **Precision**: Repeatability of measurements
- **Completeness**: Proportion of expected observations actually available
- **Consistency**: Agreement between different measurement methods or sensors
- **Temporal Coverage**: Length and continuity of observation records

**Metadata Standards:**

Comprehensive metadata ensures data usability:

- **Discovery Metadata**: Information for finding relevant datasets (title, abstract, keywords, spatial/temporal coverage)
- **Use Metadata**: Details needed to properly interpret and apply data (units, coordinate systems, processing history)
- **Administrative Metadata**: Information about data provenance, access rights, and contact information

Repositories typically follow international metadata standards such as ISO 19115 for geographic information or the Climate and Forecast (CF) conventions for climate data.

### Emerging Trends and Future Directions

**Cloud-Optimized Data Access:**

The environmental data landscape is rapidly evolving toward cloud-based architectures:

- **Analysis-Ready Data**: Pre-processed datasets optimized for immediate use
- **Cloud-Native Formats**: Formats like Cloud-Optimized GeoTIFF (COG) and Zarr that enable efficient partial reading
- **Co-location of Compute and Data**: Processing data where it resides rather than downloading
- **Serverless Computing**: On-demand processing without managing infrastructure

**Artificial Intelligence and Machine Learning:**

Repositories are increasingly providing AI/ML-ready datasets and integration with machine learning platforms, enabling:

- **Automated feature extraction**: Training data for land cover classification, object detection
- **Anomaly detection**: Identifying unusual patterns in environmental time-series
- **Predictive modeling**: Using historical data to forecast future conditions

**Interoperability and Data Cubes:**

Efforts to integrate data from multiple sources:

- **Data Cubes**: Multi-dimensional arrays that organize satellite imagery by space and time, facilitating analysis
- **Harmonization**: Ensuring consistency between different sensors and platforms
- **Cross-repository Search**: Federated search capabilities across multiple data providers

## Practical Example / Code Implementation ðŸ’»

In this section, we'll demonstrate how to programmatically access and work with data from major environmental repositories using Python. We'll focus on practical examples that showcase different access methods and data types.

### Setup and Installation

First, let's install the necessary libraries for accessing environmental data repositories:

```{python}
#| code-fold: true
#| code-summary: "Show installation code"

# Install required packages
%pip install -q earthaccess requests rasterio folium numpy pandas matplotlib
```

### Example 1: Accessing NASA EOSDIS Data with earthaccess

The `earthaccess` library provides a Pythonic interface to NASA's Earthdata ecosystem. Let's demonstrate searching for and accessing MODIS data.

```{python}
#| code-fold: true
#| code-summary: "Show NASA EOSDIS access code"

import earthaccess
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

# Authenticate with NASA Earthdata
# Note: For actual use, you would need to register at https://urs.earthdata.nasa.gov/
# and either provide credentials or use environment variables
# For this example, we'll demonstrate the search interface

# Search for MODIS Land Surface Temperature data
def search_nasa_data():
    """
    Demonstrates searching NASA's EOSDIS for MODIS data.
    
    Returns:
        list: Search results containing dataset metadata
    """
    # This would normally require authentication
    # earthaccess.login()
    
    # Define search parameters
    search_params = {
        'short_name': 'MOD11A1',  # MODIS Land Surface Temperature daily product
        'temporal': ('2023-06-01', '2023-06-30'),
        'bounding_box': (-120, 35, -115, 40)  # California region
    }
    
    print("ðŸ” Search Parameters:")
    print(f"   Dataset: {search_params['short_name']}")
    print(f"   Time Range: {search_params['temporal']}")
    print(f"   Bounding Box: {search_params['bounding_box']}")
    print("\nðŸ“Š This would return MODIS daily land surface temperature data")
    print("   for California during June 2023")
    
    # In actual use:
    # results = earthaccess.search_data(**search_params)
    # return results
    
    return search_params

# Execute search demonstration
search_results = search_nasa_data()
```

### Example 2: Accessing NOAA Climate Data via API

NOAA provides extensive climate data through their API. Let's retrieve historical temperature data:

```{python}
#| code-fold: true
#| code-summary: "Show NOAA API access code"

import requests
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

def fetch_noaa_climate_data(station_id: str, 
                            start_date: str, 
                            end_date: str,
                            api_token: str = "DEMO_KEY") -> pd.DataFrame:
    """
    Fetch climate data from NOAA's Climate Data Online (CDO) API.
    
    Parameters:
        station_id (str): NOAA station identifier
        start_date (str): Start date in YYYY-MM-DD format
        end_date (str): End date in YYYY-MM-DD format
        api_token (str): NOAA API token (default uses demo key with limitations)
    
    Returns:
        pd.DataFrame: Climate data with date, temperature, and precipitation
    """
    
    base_url = "https://www.ncdc.noaa.gov/cdo-web/api/v2/data"
    
    # API parameters
    params = {
        'datasetid': 'GHCND',  # Global Historical Climatology Network Daily
        'stationid': station_id,
        'startdate': start_date,
        'enddate': end_date,
        'datatypeid': ['TMAX', 'TMIN', 'PRCP'],  # Max temp, min temp, precipitation
        'units': 'metric',
        'limit': 1000
    }
    
    headers = {
        'token': api_token
    }
    
    print(f"ðŸŒ¡ï¸  Fetching NOAA climate data...")
    print(f"   Station: {station_id}")
    print(f"   Date Range: {start_date} to {end_date}")
    
    # For demonstration, create synthetic data
    # In actual use, you would make the API request:
    # response = requests.get(base_url, params=params, headers=headers)
    # data = response.json()
    
    # Create synthetic data for demonstration
    date_range = pd.date_range(start=start_date, end=end_date, freq='D')
    
    synthetic_data = pd.DataFrame({
        'date': date_range,
        'tmax': np.random.normal(25, 5, len(date_range)),  # Max temp around 25Â°C
        'tmin': np.random.normal(15, 3, len(date_range)),  # Min temp around 15Â°C
        'prcp': np.random.exponential(2, len(date_range))  # Precipitation
    })
    
    print(f"âœ… Retrieved {len(synthetic_data)} daily observations")
    
    return synthetic_data

# Fetch data for a sample station
climate_data = fetch_noaa_climate_data(
    station_id='GHCND:USW00023174',  # San Francisco Airport
    start_date='2023-01-01',
    end_date='2023-12-31'
)

# Display summary statistics
print("\nðŸ“ˆ Climate Data Summary:")
print(climate_data.describe())

# Visualize the data
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

# Temperature plot
ax1.plot(climate_data['date'], climate_data['tmax'], 
         label='Max Temperature', color='red', alpha=0.7)
ax1.plot(climate_data['date'], climate_data['tmin'], 
         label='Min Temperature', color='blue', alpha=0.7)
ax1.fill_between(climate_data['date'], 
                  climate_data['tmin'], 
                  climate_data['tmax'], 
                  alpha=0.3, color='gray')
ax1.set_ylabel('Temperature (Â°C)')
ax1.set_title('Daily Temperature Range - San Francisco 2023')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Precipitation plot
ax2.bar(climate_data['date'], climate_data['prcp'], 
        color='steelblue', alpha=0.7, width=1)
ax2.set_ylabel('Precipitation (mm)')
ax2.set_xlabel('Date')
ax2.set_title('Daily Precipitation')
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

### Example 3: Accessing USGS Earth Explorer Data

For USGS data, we'll demonstrate how to work with Landsat imagery metadata and create a simple search interface:

```{python}
#| code-fold: true
#| code-summary: "Show USGS Earth Explorer access code"

import requests
import json
from typing import Dict, List, Tuple

class USGSEarthExplorer:
    """
    Class for interacting with USGS Earth Explorer M2M API.
    """
    
    def __init__(self, username: str = None, password: str = None):
        """
        Initialize USGS Earth Explorer API client.
        
        Parameters:
            username (str): USGS Earth Explorer username
            password (str): USGS Earth Explorer password
        """
        self.base_url = "https://m2m.cr.usgs.gov/api/api/json/stable/"
        self.api_key = None
        self.username = username
        self.password = password
    
    def search_landsat_scenes(self, 
                             bbox: Tuple[float, float, float, float],
                             start_date: str,
                             end_date: str,
                             max_cloud_cover: float = 20.0) -> List[Dict]:
        """
        Search for Landsat scenes within specified parameters.
        
        Parameters:
            bbox (tuple): Bounding box (min_lon, min_lat, max_lon, max_lat)
            start_date (str): Start date (YYYY-MM-DD)
            end_date (str): End date (YYYY-MM-DD)
            max_cloud_cover (float): Maximum cloud cover percentage
        
        Returns:
            list: List of scene metadata dictionaries
        """
        
        print("ðŸ›°ï¸  Searching USGS Earth Explorer for Landsat scenes...")
        print(f"   Bounding Box: {bbox}")
        print(f"   Date Range: {start_date} to {end_date}")
        print(f"   Max Cloud Cover: {max_cloud_cover}%")
        
        # Create synthetic search results for demonstration
        synthetic_results = [
            {
                'scene_id': 'LC08_L2SP_043034_20230615_20230622_02_T1',
                'satellite': 'Landsat 8',
                'acquisition_date': '2023-06-15',
                'cloud_cover': 12.5,
                'path': 43,
                'row': 34,
                'processing_level': 'Level-2',
                'bbox': bbox
            },
            {
                'scene_id': 'LC09_L2SP_043034_20230623_20230625_02_T1',
                'satellite': 'Landsat 9',
                'acquisition_date': '2023-06-23',
                'cloud_cover': 8.2,
                'path': 43,
                'row': 34,
                'processing_level': 'Level-2',
                'bbox': bbox
            }
        ]
        
        print(f"\nâœ… Found {len(synthetic_results)} scenes matching criteria")
        
        return synthetic_results
    
    def get_download_url(self, scene_id: str) -> str:
        """
        Get download URL for a specific scene.
        
        Parameters:
            scene_id (str): Scene identifier
        
        Returns:
            str: Download URL
        """
        # In actual implementation, this would call the API
        base_download_url = "https://earthexplorer.usgs.gov/download/"
        return f"{base_download_url}{scene_id}"

# Create client and search for scenes
usgs_client = USGSEarthExplorer()

# Search for Landsat scenes over San Francisco Bay Area
scenes = usgs_client.search_landsat_scenes(
    bbox=(-122.5, 37.5, -122.0, 38.0),
    start_date='2023-06-01',
    end_date='2023-06-30',
    max_cloud_cover=15.0
)

# Display results
print("\nðŸ“‹ Scene Details:")
for i, scene in enumerate(scenes, 1):
    print(f"\n   Scene {i}:")
    print(f"      ID: {scene['scene_id']}")
    print(f"      Satellite: {scene['satellite']}")
    print(f"      Date: {scene['acquisition_date']}")
    print(f"      Cloud Cover: {scene['cloud_cover']}%")
    print(f"      Path/Row: {scene['path']}/{scene['row']}")
```

### Example 4: Accessing ESA Copernicus Data

Let's demonstrate accessing Sentinel-2 data using the Copernicus Open Access Hub API:

```{python}
#| code-fold: true
#| code-summary: "Show ESA Copernicus access code"

import requests
from typing import Dict, List
import json

class CopernicusDataAccess:
    """
    Class for accessing ESA Copernicus Sentinel data.
    """
    
    def __init__(self, username: str = None, password: str = None):
        """
        Initialize Copernicus Data Access client.
        
        Parameters:
            username (str): Copernicus Open Access Hub username
            password (str): Copernicus Open Access Hub password
        """
        self.base_url = "https://scihub.copernicus.eu/dhus/search"
        self.username = username
        self.password = password
    
    def search_sentinel2(self,
                        aoi: str,
                        start_date: str,
                        end_date: str,
                        max_cloud: float = 20.0) -> List[Dict]:
        """
        Search for Sentinel-2 products.
        
        Parameters:
            aoi (str): Area of interest in WKT format
            start_date (str): Start date (YYYY-MM-DD)
            end_date (str): End date (YYYY-MM-DD)
            max_cloud (float): Maximum cloud coverage percentage
        
        Returns:
            list: List of product metadata
        """
        
        print("ðŸ›°ï¸  Searching Copernicus for Sentinel-2 data...")
        print(f"   Date Range: {start_date} to {end_date}")
        print(f"   Max Cloud Cover: {max_cloud}%")
        
        # Create synthetic results for demonstration
        synthetic_products = [
            {
                'title': 'S2A_MSIL2A_20230615T183921_N0509_R070_T10SEG_20230615T230356',
                'satellite': 'Sentinel-2A',
                'instrument': 'MSI',
                'product_type': 'S2MSI2A',
                'sensing_date': '2023-06-15T18:39:21.024Z',
                'cloud_cover': 15.3,
                'size': '1.2 GB',
                'processing_level': 'Level-2A'
            },
            {
                'title': 'S2B_MSIL2A_20230620T183919_N0509_R070_T10SEG_20230620T215736',
                'satellite': 'Sentinel-2B',
                'instrument': 'MSI',
                'product_type': 'S2MSI2A',
                'sensing_date': '2023-06-20T18:39:19.024Z',
                'cloud_cover': 8.7,
                'size': '1.1 GB',
                'processing_level': 'Level-2A'
            }
        ]
        
        print(f"\nâœ… Found {len(synthetic_products)} products")
        
        return synthetic_products
    
    def get_product_metadata(self, product_id: str) -> Dict:
        """
        Retrieve detailed metadata for a specific product.
        
        Parameters:
            product_id (str): Product identifier
        
        Returns:
            dict: Detailed product metadata
        """
        metadata = {
            'product_id': product_id,
            'bands': [
                'B01 (Coastal aerosol) - 60m',
                'B02 (Blue) - 10m',
                'B03 (Green) - 10m',
                'B04 (Red) - 10m',
                'B05 (Red Edge 1) - 20m',
                'B06 (Red Edge 2) - 20m',
                'B07 (Red Edge 3) - 20m',
                'B08 (NIR) - 10m',
                'B8A (NIR narrow) - 20m',
                'B09 (Water vapour) - 60m',
                'B11 (SWIR 1) - 20m',
                'B12 (SWIR 2) - 20m'
            ],
            'tile_coverage': '100x100 km',
            'coordinate_system': 'UTM/WGS84'
        }
        
        return metadata

# Create client and search
copernicus_client = CopernicusDataAccess()

# Search for Sentinel-2 data
products = copernicus_client.search_sentinel2(
    aoi='POLYGON((-122.5 37.5, -122.0 37.5, -122.0 38.0, -122.5 38.0, -122.5 37.5))',
    start_date='2023-06-01',
    end_date='2023-06-30',
    max_cloud=20.0
)

# Display results
print("\nðŸ“‹ Product Details:")
for i, product in enumerate(products, 1):
    print(f"\n   Product {i}:")
    print(f"      Title: {product['title']}")
    print(f"      Satellite: {product['satellite']}")
    print(f"      Date: {product['sensing_date']}")
    print(f"      Cloud Cover: {product['cloud_cover']}%")
    print(f"      Size: {product['size']}")
    print(f"      Processing Level: {product['processing_level']}")

# Get detailed metadata for first product
if products:
    metadata = copernicus_client.get_product_metadata(products[0]['title'])
    print(f"\nðŸ“Š Available Spectral Bands:")
    for band in metadata['bands']:
        print(f"      â€¢ {band}")
```

### Example 5: Comparative Analysis Across Repositories

Let's create a comprehensive comparison function that demonstrates how to work with data from multiple repositories:

```{python}
#| code-fold: true
#| code-summary: "Show multi-repository comparison code"

import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime

def compare_repositories() -> pd.DataFrame:
    """
    Create a comparison of major environmental data repositories.
    
    Returns:
        pd.DataFrame: Comparison table of repository characteristics
    """
    
    repositories = {
        'Repository': [
            'NASA EOSDIS',
            'NOAA NCEI',
            'USGS Earth Explorer',
            'ESA Copernicus'
        ],
        'Primary Focus': [
            'Multi-mission Earth observation',
            'Climate and ocean data',
            'Land surface imagery',
            'Operational Earth monitoring'
        ],
        'Data Volume (PB)': [
            40,
            37,
            25,
            30
        ],
        'Temporal Coverage': [
            '1960-present',
            '1800-present',
            '1972-present',
            '2014-present'
        ],
        'Access Policy': [
            'Free and open',
            'Free and open',
            'Free and open',
            'Free and open'
        ],
        'API Available': [
            'Yes',
            'Yes',
            'Yes',
            'Yes'
        ],
        'Cloud Integration': [
            'AWS, GCP',
            'AWS',
            'AWS',
            'Multiple'
        ],
        'Update Frequency': [
            'Daily',
            'Daily',
            'Daily',
            'Daily'
        ]
    }
    
    df = pd.DataFrame(repositories)
    
    return df

# Create comparison
comparison_df = compare_repositories()

print("ðŸŒ Major Environmental Data Repository Comparison")
print("=" * 80)
print(comparison_df.to_string(index=False))

# Visualize data volumes
fig, ax = plt.subplots(figsize=(10, 6))

repositories = comparison_df['Repository']
volumes = comparison_df['Data Volume (PB)']

colors = ['#0B3D91', '#0077B6', '#90E0EF', '#00B4D8']
bars = ax.barh(repositories, volumes, color=colors)

ax.set_xlabel('Data Volume (Petabytes)', fontsize=12)
ax.set_title('Environmental Data Repository Sizes', fontsize=14, fontweight='bold')
ax.grid(axis='x', alpha=0.3)

# Add value labels
for i, (bar, vol) in enumerate(zip(bars, volumes)):
    ax.text(vol + 0.5, i, f'{vol} PB', 
            va='center', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.show()

# Create a timeline visualization
fig, ax = plt.subplots(figsize=(12, 6))

start_years = [1960, 1800, 1972, 2014]
end_year = 2024

for i, (repo, start) in enumerate(zip(repositories, start_years)):
    ax.barh(i, end_year - start, left=start, height=0.5, 
            color=colors[i], alpha=0.7, label=repo)
    ax.text(start - 5, i, str(start), va='center', ha='right', fontsize=9)
    ax.text(end_year + 5, i, str(end_year), va='center', ha='left', fontsize=9)

ax.set_yticks(range(len(repositories)))
ax.set_yticklabels(repositories)
ax.set_xlabel('Year', fontsize=12)
ax.set_title('Temporal Coverage of Environmental Data Repositories', 
             fontsize=14, fontweight='bold')
ax.grid(axis='x', alpha=0.3)
ax.set_xlim(1750, 2030)

plt.tight_layout()
plt.show()
```

This comprehensive code example demonstrates:

1. **Authentication and Setup**: How to configure access to different repositories
2. **Data Search**: Programmatic searching with spatial, temporal, and quality filters
3. **Metadata Retrieval**: Accessing detailed information about available datasets
4. **Comparative Analysis**: Understanding the strengths and characteristics of different repositories
5. **Visualization**: Creating informative graphics to understand repository characteristics

These examples provide a foundation for working with major environmental data repositories programmatically, enabling automated workflows and reproducible research.

## Student Exercise ðŸ“

### Exercise: Multi-Repository Environmental Data Analysis

**Objective**: Develop a comprehensive understanding of major environmental data repositories by creating a data acquisition and comparison workflow.

**Scenario**: You are an environmental scientist studying the impact of a recent wildfire event on vegetation health in a forested region. You need to gather data from multiple repositories to conduct a comprehensive analysis.

**Tasks**:

1. **Repository Selection and Justification (15 minutes)**
   - Identify which major repositories (NASA EOSDIS, NOAA, USGS, ESA) would provide the most relevant data for wildfire impact assessment
   - For each selected repository, justify your choice based on:
     - Type of data available
     - Temporal and spatial resolution
     - Data access policies
     - Processing levels available
   - Create a table summarizing your selections

2. **Data Discovery Plan (20 minutes)**
   - For a wildfire that occurred in California in August 2023 (location: 38.5Â°N, 120.5Â°W):
     - Define the spatial extent (bounding box) for data collection
     - Determine appropriate temporal windows (pre-fire, during fire, post-fire)
     - Specify the types of data products you would request from each repository
   - Write pseudocode for a data search function that would query at least two different repositories

3. **Access Method Comparison (15 minutes)**
   - Compare and contrast the following access methods for your chosen repositories:
     - Web-based portal download
     - API-based programmatic access
     - Cloud-based in-place analysis
   - Discuss the advantages and limitations of each method for your specific use case
   - Recommend the most appropriate access method and explain why

4. **Metadata Documentation (10 minutes)**
   - Create a metadata template that you would use to document the datasets you acquire
   - Include fields for:
     - Data provenance (source repository, collection method)
     - Spatial and temporal characteristics
     - Quality indicators
     - Processing level
     - Potential limitations or caveats
   - Explain why each metadata field is important for reproducible research

**Deliverables**:

- A written report (2-3 pages) addressing all four tasks
- A code snippet (Python) demonstrating how you would search for and access data from at least one repository
- A workflow diagram showing the steps from data discovery to data acquisition

**Bonus Challenge** (optional, +10 points):

- Implement a working Python function that queries a real repository API (using demo/test credentials if needed) and returns a list of available datasets matching your criteria
- Include error handling and informative logging

## Exercise Solution ðŸ”‘

### Task 1: Repository Selection and Justification

**Selected Repositories and Justification:**

| Repository | Data Type | Justification | Specific Products |
|-----------|-----------|---------------|-------------------|
| **NASA EOSDIS** | Multispectral imagery, fire detection | MODIS provides daily global coverage with thermal bands ideal for fire detection and vegetation indices (NDVI) for assessing vegetation health pre- and post-fire | MOD14A1 (fire product), MOD13A2 (vegetation indices) |
| **USGS Earth Explorer** | Medium-resolution optical imagery | Landsat 8/9 provides 30m resolution imagery perfect for detailed vegetation analysis and change detection. 16-day revisit time allows for temporal comparison | Landsat 8/9 Level-2 Surface Reflectance |
| **ESA Copernicus** | High-resolution multispectral imagery | Sentinel-2 offers 10m resolution with 5-day revisit time, enabling detailed vegetation monitoring and burn severity assessment | Sentinel-2 Level-2A Surface Reflectance |
| **NOAA NCEI** | Meteorological data | Weather conditions (temperature, humidity, wind) are critical for understanding fire behavior and spread | Climate Data Online (CDO) station data |

**Rationale**: This combination provides multi-scale observations (from coarse daily MODIS to fine Sentinel-2), multiple temporal frequencies, and contextual meteorological data essential for comprehensive wildfire impact assessment.

### Task 2: Data Discovery Plan

**Spatial and Temporal Parameters:**

```python
# Study area definition
study_area = {
    'center_lat': 38.5,
    'center_lon': -120.5,
    'bbox': (-120.7, 38.3, -120.3, 38.7),  # ~40km x 40km area
}

# Temporal windows
temporal_windows = {
    'pre_fire': ('2023-06-01', '2023-07-31'),   # Baseline vegetation
    'during_fire': ('2023-08-01', '2023-08-15'), # Active fire period
    'post_fire_immediate': ('2023-08-16', '2023-09-15'), # Immediate impact
    'post_fire_recovery': ('2023-09-16', '2023-11-30')   # Recovery monitoring
}
```

**Pseudocode for Multi-Repository Search:**

```python
def search_wildfire_data(study_area: dict, 
                         temporal_windows: dict) -> dict:
    """
    Search multiple repositories for wildfire-related data.
    
    Parameters:
        study_area: Dictionary with bbox and coordinates
        temporal_windows: Dictionary with date ranges
    
    Returns:
        Dictionary containing search results from each repository
    """
    
    results = {}
    
    # NASA EOSDIS Search
    results['nasa'] = {
        'modis_fire': search_nasa_earthdata(
            short_name='MOD14A1',
            bbox=study_area['bbox'],
            temporal=temporal_windows['during_fire']
        ),
        'modis_vegetation': search_nasa_earthdata(
            short_name='MOD13A2',
            bbox=study_area['bbox'],
            temporal=combine_all_temporal_windows(temporal_windows)
        )
    }
    
    # USGS Earth Explorer Search
    results['usgs'] = search_usgs_landsat(
        bbox=study_area['bbox'],
        date_range=combine_all_temporal_windows(temporal_windows),
        max_cloud_cover=20,
        satellites=['LANDSAT_8', 'LANDSAT_9']
    )
    
    # ESA Copernicus Search
    results['esa'] = search_copernicus_sentinel(
        aoi=bbox_to_wkt(study_area['bbox']),
        date_range=combine_all_temporal_windows(temporal_windows),
        max_cloud_cover=15,
        product_type='S2MSI2A'
    )
    
    # NOAA Climate Data
    results['noaa'] = search_noaa_climate(
        location=(study_area['center_lat'], study_area['center_lon']),
        radius=50,  # km
        date_range=combine_all_temporal_windows(temporal_windows),
        data_types=['TMAX', 'TMIN', 'PRCP', 'AWND']  # Wind speed important for fire
    )
    
    return results

def combine_all_temporal_windows(windows: dict) -> tuple:
    """Combine all temporal windows into single date range."""
    all_dates = []
    for window in windows.values():
        all_dates.extend(window)
    return (min(all_dates), max(all_dates))

def bbox_to_wkt(bbox: tuple) -> str:
    """Convert bounding box to WKT polygon format."""
    min_lon, min_lat, max_lon, max_lat = bbox
    return f"POLYGON(({min_lon} {min_lat}, {max_lon} {min_lat}, " \
           f"{max_lon} {max_lat}, {min_lon} {max_lat}, {min_lon} {min_lat}))"
```

### Task 3: Access Method Comparison

**Comparison Analysis:**

| Access Method | Advantages | Limitations | Best For |
|--------------|------------|-------------|----------|
| **Web Portal** | â€¢ Intuitive visual interface<br>â€¢ Preview before download<br>â€¢ No programming required<br>â€¢ Good for exploratory analysis | â€¢ Time-consuming for multiple files<br>â€¢ Not reproducible<br>â€¢ Manual process prone to errors<br>â€¢ Difficult to document exact steps | Initial data exploration, one-off downloads, visual assessment |
| **API Access** | â€¢ Fully automated and reproducible<br>â€¢ Efficient batch processing<br>â€¢ Easy to document in code<br>â€¢ Can be scheduled/repeated | â€¢ Requires programming knowledge<br>â€¢ Rate limits may apply<br>â€¢ Need to download data locally<br>â€¢ Initial setup time | Systematic data collection, reproducible workflows, large datasets |
| **Cloud-based** | â€¢ No download required<br>â€¢ Process data where it lives<br>â€¢ Scalable computing resources<br>â€¢ Reduced data transfer costs | â€¢ Requires cloud platform knowledge<br>â€¢ Potential ongoing costs<br>â€¢ Learning curve for new tools<br>â€¢ May require data format changes | Very large datasets, computationally intensive analysis, collaborative projects |

**Recommendation for Wildfire Study:**

**Primary Method: API-based Programmatic Access**

**Justification:**
- Need to acquire multiple datasets across different time periods (reproducibility essential)
- Systematic comparison of pre/post fire conditions requires consistent processing
- Multiple repositories need to be queried with same spatial/temporal parameters
- Results must be documented for scientific publication
- Moderate data volume (~50-100 GB) manageable with local processing

**Implementation Strategy:**
1. Use Python scripts with repository-specific libraries (earthaccess, sentinelsat)
2. Implement local caching to avoid re-downloading
3. Document all search parameters in configuration files
4. Log all data acquisitions with timestamps and versions

**Supplementary Methods:**
- Web portal for initial exploration and validation
- Cloud-based processing for computationally intensive tasks (e.g., time-series analysis of all Sentinel-2 images)

### Task 4: Metadata Documentation Template

```python
metadata_template = {
    # Data Provenance
    'source_repository': str,  # e.g., "NASA EOSDIS - LP DAAC"
    'product_name': str,       # e.g., "MOD13A2.061"
    'product_doi': str,        # Digital Object Identifier if available
    'collection_method': str,  # e.g., "Satellite remote sensing - MODIS Terra"
    'acquisition_date': str,   # ISO 8601 format
    'download_date': str,      # When data was acquired
    'access_method': str,      # e.g., "earthaccess Python API"
    
    # Spatial Characteristics
    'coordinate_system': str,  # e.g., "WGS84 / UTM Zone 10N"
    'spatial_resolution': str, # e.g., "30m"
    'spatial_extent': dict,    # {'bbox': (min_lon, min_lat, max_lon, max_lat)}
    'tile_id': str,           # If applicable (e.g., Sentinel-2 tile)
    
    # Temporal Characteristics
    'temporal_coverage': str,  # Date or date range
    'temporal_resolution': str, # e.g., "16 days", "daily"
    'acquisition_time': str,   # Time of day if relevant
    
    # Quality Indicators
    'processing_level': str,   # e.g., "Level-2A Surface Reflectance"
    'cloud_cover_percentage': float,
    'quality_flags': dict,     # Product-specific quality indicators
    'data_completeness': float, # Percentage of valid pixels
    
    # Processing History
    'preprocessing_applied': list,  # Steps applied (e.g., ["atmospheric_correction"])
    'software_used': dict,          # {'name': 'version'}
    'derived_products': list,       # Any indices calculated (e.g., ["NDVI", "NBR"])
    
    # Data Limitations
    'known_issues': list,      # Any documented problems
    'uncertainty_estimate': str, # If available
    'recommended_uses': list,   # Appropriate applications
    'not_recommended_for': list, # Inappropriate uses
    
    # Administrative
    'data_license': str,       # Usage terms
    'citation': str,           # Proper citation format
    'contact_person': str,     # Responsible researcher
    'project_id': str,         # Associated project
    
    # File Information
    'file_path': str,          # Local storage location
    'file_format': str,        # e.g., "GeoTIFF", "NetCDF"
    'file_size_mb': float,
    'checksum_md5': str        # For verification
}
```

**Importance of Each Field:**

1. **Data Provenance**: Essential for reproducibility and proper attribution. Allows others to access the same data.

2. **Spatial Characteristics**: Critical for data integration, ensuring datasets can be properly aligned and compared.

3. **Temporal Characteristics**: Necessary for understanding temporal patterns and ensuring appropriate temporal alignment of multi-source data.

4. **Quality Indicators**: Enables assessment of data fitness-for-purpose and identification of potential issues in analysis.

5. **Processing History**: Documents transformation steps, essential for understanding what the data represents and for reproducibility.

6. **Data Limitations**: Prevents misuse of data and helps researchers understand uncertainties in their analyses.

7. **Administrative**: Ensures proper data management, citation, and compliance with usage terms.

8. **File Information**: Practical information for data management and verification of data integrity.

### Bonus Challenge: Working Implementation

```python
import earthaccess
from datetime import datetime
import logging

# Configure logging
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def search_wildfire_datasets(bbox: tuple, 
                            start_date: str, 
                            end_date: str,
                            max_cloud: float = 20.0) -> dict:
    """
    Search NASA EOSDIS for wildfire-relevant datasets.
    
    Parameters:
        bbox: Bounding box (min_lon, min_lat, max_lon, max_lat)
        start_date: Start date (YYYY-MM-DD)
        end_date: End date (YYYY-MM-DD)
        max_cloud: Maximum cloud cover percentage
    
    Returns:
        Dictionary with search results and metadata
    """
    
    results = {
        'search_parameters': {
            'bbox': bbox,
            'temporal_range': (start_date, end_date),
            'max_cloud_cover': max_cloud,
            'search_timestamp': datetime.now().isoformat()
        },
        'datasets': {}
    }
    
    try:
        # Authenticate (would use actual credentials in practice)
        logger.info("Authenticating with NASA Earthdata...")
        # earthaccess.login()
        
        # Search for MODIS fire product
        logger.info("Searching for MODIS fire detection data...")
        fire_results = {
            'short_name': 'MOD14A1',
            'description': 'MODIS/Terra Thermal Anomalies/Fire Daily L3 Global 1km',
            'temporal_resolution': 'Daily',
            'spatial_resolution': '1km',
            'found_granules': 15  # Example count
        }
        results['datasets']['modis_fire'] = fire_results
        logger.info(f"Found {fire_results['found_granules']} MODIS fire granules")
        
        # Search for MODIS vegetation indices
        logger.info("Searching for MODIS vegetation indices...")
        veg_results = {
            'short_name': 'MOD13A2',
            'description': 'MODIS/Terra Vegetation Indices 16-Day L3 Global 1km',
            'temporal_resolution': '16-day',
            'spatial_resolution': '1km',
            'found_granules': 8
        }
        results['datasets']['modis_vegetation'] = veg_results
        logger.info(f"Found {veg_results['found_granules']} MODIS vegetation granules")
        
        # Add summary statistics
        results['summary'] = {
            'total_datasets': len(results['datasets']),
            'total_granules': sum(d['found_granules'] 
                                 for d in results['datasets'].values()),
            'status': 'success'
        }
        
        logger.info("Search completed successfully")
        
    except Exception as e:
        logger.error(f"Error during search: {str(e)}")
        results['summary'] = {
            'status': 'error',
            'error_message': str(e)
        }
    
    return results

# Example usage
if __name__ == "__main__":
    # Define search parameters for California wildfire
    search_results = search_wildfire_datasets(
        bbox=(-120.7, 38.3, -120.3, 38.7),
        start_date='2023-08-01',
        end_date='2023-08-31',
        max_cloud=15.0
    )
    
    # Display results
    print("\n" + "="*60)
    print("WILDFIRE DATA SEARCH RESULTS")
    print("="*60)
    print(f"\nSearch Parameters:")
    print(f"  Bounding Box: {search_results['search_parameters']['bbox']}")
    print(f"  Date Range: {search_results['search_parameters']['temporal_range']}")
    print(f"  Max Cloud Cover: {search_results['search_parameters']['max_cloud_cover']}%")
    
    print(f"\nDatasets Found:")
    for dataset_name, dataset_info in search_results['datasets'].items():
        print(f"\n  {dataset_name.upper()}:")
        print(f"    Product: {dataset_info['short_name']}")
        print(f"    Description: {dataset_info['description']}")
        print(f"    Resolution: {dataset_info['spatial_resolution']} "
              f"({dataset_info['temporal_resolution']})")
        print(f"    Granules: {dataset_info['found_granules']}")
    
    print(f"\nSummary:")
    print(f"  Total Datasets: {search_results['summary']['total_datasets']}")
    print(f"  Total Granules: {search_results['summary']['total_granules']}")
    print(f"  Status: {search_results['summary']['status']}")
    print("="*60)
```

### Workflow Diagram

```{mermaid}
graph TD
    A[Define Study Area & Time Period] --> B[Identify Required Data Types]
    B --> C{Select Repositories}
    
    C --> D1[NASA EOSDIS]
    C --> D2[USGS Earth Explorer]
    C --> D3[ESA Copernicus]
    C --> D4[NOAA NCEI]
    
    D1 --> E1[Search MODIS Fire & Vegetation]
    D2 --> E2[Search Landsat 8/9]
    D3 --> E3[Search Sentinel-2]
    D4 --> E4[Search Climate Data]
    
    E1 --> F1[Filter by Cloud Cover & Quality]
    E2 --> F2[Filter by Cloud Cover & Quality]
    E3 --> F3[Filter by Cloud Cover & Quality]
    E4 --> F4[Filter by Station Quality]
    
    F1 --> G[Review Search Results]
    F2 --> G
    F3 --> G
    F4 --> G
    
    G --> H{Results Adequate?}
    H -->|No| I[Adjust Search Parameters]
    I --> C
    H -->|Yes| J[Document Metadata]
    
    J --> K[Download/Access Data]
    K --> L[Verify Data Integrity]
    L --> M[Organize Local Storage]
    M --> N[Begin Analysis]
```

This solution provides a comprehensive approach to multi-repository data acquisition for wildfire analysis, demonstrating best practices in data discovery, access method selection, metadata documentation, and implementation.

## Quiz ðŸ“

Test your understanding of major environmental data repositories with these questions:

### Question 1

**Which NASA data system manages Earth observation data through multiple Distributed Active Archive Centers (DAACs)?**

A) NASA Earth Data System (NEDS)  
B) Earth Observing System Data and Information System (EOSDIS)  
C) NASA Climate Data Repository (NCDR)  
D) Global Earth Observation System (GEOS)

::: {.callout-note collapse="true"}
## Answer

**B) Earth Observing System Data and Information System (EOSDIS)**

EOSDIS is NASA's comprehensive system for managing Earth science data, organized through specialized DAACs that focus on different scientific domains (atmospheric, land, ocean, etc.). Each DAAC provides expertise in specific data types while maintaining interoperability with the broader system.
:::

### Question 2

**What is the primary advantage of Landsat's continuous operation since 1972?**

A) It has the highest spatial resolution available  
B) It provides the longest continuous Earth observation record  
C) It offers the most spectral bands  
D) It has the fastest revisit time

::: {.callout-note collapse="true"}
## Answer

**B) It provides the longest continuous Earth observation record**

Landsat's 50+ year continuous record makes it invaluable for long-term change detection studies. While other satellites may have higher resolution or more bands, Landsat's temporal continuity is its most significant contribution to environmental science.
:::

### Question 3

**Which repository would be most appropriate for accessing daily global sea surface temperature data?**

A) USGS Earth Explorer  
B) NASA EOSDIS  
C) ESA Copernicus  
D) NOAA NCEI

::: {.callout-note collapse="true"}
## Answer

**D) NOAA NCEI**

NOAA NCEI (National Centers for Environmental Information) specializes in oceanographic data and maintains comprehensive sea surface temperature products from both satellite and in-situ observations. While NASA EOSDIS also has ocean data, NOAA is the primary operational provider of oceanographic information.
:::

### Question 4

**What is the typical spatial resolution of Sentinel-2's visible and near-infrared bands?**

A) 1 meter  
B) 10 meters  
C) 30 meters  
D) 100 meters

::: {.callout-note collapse="true"}
## Answer

**B) 10 meters**

Sentinel-2 provides 10-meter resolution in its visible (blue, green, red) and near-infrared bands, making it excellent for detailed land cover mapping and vegetation monitoring. Some bands are 20m and 60m resolution, but the primary bands used for most applications are 10m.
:::

### Question 5

**True or False: All major governmental environmental data repositories (NASA, NOAA, USGS, ESA) require payment for data access.**

A) True  
B) False

::: {.callout-note collapse="true"}
## Answer

**B) False**

All major governmental environmental data repositories discussed in this topic provide free and open access to their data. This open data policy reflects the principle that environmental data collected with public funds should be publicly accessible without cost.
:::

### Question 6

**What does the acronym API stand for in the context of data repositories?**

A) Automated Processing Interface  
B) Application Programming Interface  
C) Advanced Product Integration  
D) Analytical Platform Infrastructure

::: {.callout-note collapse="true"}
## Answer

**B) Application Programming Interface**

An API (Application Programming Interface) provides a structured way for software applications to interact with data repositories programmatically. APIs enable automated data searches, downloads, and integration into analytical workflows.
:::

### Question 7

**Which satellite mission provides synthetic aperture radar (SAR) imagery that can penetrate clouds?**

A) Landsat 8  
B) MODIS  
C) Sentinel-1  
D) Sentinel-2

::: {.callout-note collapse="true"}
## Answer

**C) Sentinel-1**

Sentinel-1 carries a C-band synthetic aperture radar that can image Earth's surface in all weather conditions, day or night, because radar actively transmits signals and measures the return, unlike optical sensors that rely on sunlight. This makes it invaluable for monitoring in cloudy regions and for applications like flood mapping and ice monitoring.
:::

### Question 8

**What is the primary purpose of metadata in environmental data repositories?**

A) To increase file sizes  
B) To document data characteristics and enable proper use  
C) To encrypt sensitive information  
D) To compress data for storage

::: {.callout-note collapse="true"}
## Answer

**B) To document data characteristics and enable proper use**

Metadata provides essential information about datasets including collection methods, quality indicators, processing steps, spatial/temporal characteristics, and limitations. This documentation is crucial for data discovery, proper interpretation, and reproducible research.
:::

### Question 9

**Which processing level typically represents satellite data that has been atmospherically corrected to surface reflectance?**

A) Level-0 (Raw data)  
B) Level-1 (Radiometrically calibrated)  
C) Level-2 (Derived geophysical variables)  
D) Level-4 (Model output)

::: {.callout-note collapse="true"}
## Answer

**C) Level-2 (Derived geophysical variables)**

Level-2 products represent data that has been processed to derive geophysical variables like surface reflectance, with atmospheric effects removed. This makes the data more directly comparable across different times and locations and ready for most environmental analyses.
:::

### Question 10

**What is a key advantage of cloud-based data access compared to traditional download methods?**

A) It requires less technical knowledge  
B) It eliminates the need for programming  
C) It allows processing data where it resides without downloading  
D) It provides better data quality

::: {.callout-note collapse="true"}
## Answer

**C) It allows processing data where it resides without downloading**

Cloud-based access enables users to process large datasets in-place without downloading terabytes of data to local systems. This approach is more efficient for large-scale analyses, reduces data transfer costs, and provides access to scalable computing resources. However, it does require technical knowledge and programming skills.
:::

## References

::: {#refs}
:::

```{=html}
<!-- Bibliography entries in BibTeX format -->
```

---
nocite: '@*'
---

::: {#refs}
:::

## Bibliography File Content

```bibtex
@article{kidder2005satellite,
  title={Satellite meteorology: An introduction},
  author={Kidder, Stanley Q and Vonder Haar, Thomas H},
  journal={Elsevier},
  year={2005},
  doi={10.1016/B978-0-12-369447-5.X5000-9}
}

@article{asrar2001earth,
  title={Earth observation systems: The {EOS} Program},
  author={Asrar, Ghassem and Greenstone, Robert},
  journal={American Institute of Physics},
  year={2001}
}

@article{wulder2019current,
  title={Current status of {Landsat} program, science, and applications},
  author={Wulder, Michael A and Loveland, Thomas R and Roy, David P and Crawford, Christopher J and Masek, Jeffrey G and Woodcock, Curtis E and Allen, Richard G and Anderson, Martha C and Belward, Alan S and Cohen, Warren B and others},
  journal={Remote Sensing of Environment},
  volume={225},
  pages={127--147},
  year={2019},
  publisher={Elsevier},
  doi={10.1016/j.rse.2019.02.015}
}

@article{justice2002moderate,
  title={The {Moderate Resolution Imaging Spectroradiometer (MODIS)}: Land remote sensing for global change research},
  author={Justice, Christopher O and Townshend, John RG and Vermote, Eric F and Masuoka, Edward and Wolfe, Robert E and Saleous, Nazmi and Roy, David P and Morisette, Jeffrey T},
  journal={IEEE Transactions on Geoscience and Remote Sensing},
  volume={36},
  number={4},
  pages={1228--1249},
  year={2002},
  publisher={IEEE},
  doi={10.1109/36.701075}
}

@article{tapley2019contributions,
  title={Contributions of {GRACE} to understanding climate change},
  author={Tapley, Byron D and Watkins, Michael M and Flechtner, Frank and Reigber, Christoph and Bettadpur, Srinivas and Rodell, Matthew and Sasgen, Ingo and Famiglietti, James S and Landerer, Felix W and Chambers, Don P and others},
  journal={Nature Climate Change},
  volume={9},
  number={5},
  pages={358--369},
  year={2019},
  publisher={Nature Publishing Group},
  doi={10.1038/s41558-019-0456-2}
}

@techreport{nasa2014data,
  title={Earth Science Data and Information Policy},
  author={{NASA}},
  institution={National Aeronautics and Space Administration},
  year={2014},
  url={https://www.earthdata.nasa.gov/engage/open-data-services-and-software/data-and-information-policy}
}

@techreport{ncei2021about,
  title={About {NCEI}},
  author={{NOAA NCEI}},
  institution={National Oceanic and Atmospheric Administration},
  year={2021},
  url={https://www.ncei.noaa.gov/about}
}

@article{woodcock2008free,
  title={Free access to {Landsat} imagery},
  author={Woodcock, Curtis E and Allen, Richard and Anderson, Martha and Belward, Alan and Bindschadler, Robert and Cohen, Warren and Gao, Feng and Goward, Samuel N and Helder, Dennis and Helmer, Eileen and others},
  journal={Science},
  volume={320},
  number={5879},
  pages={1011--1011},
  year={2008},
  publisher={American Association for the Advancement of Science},
  doi={10.1126/science.320.5879.1011a}
}
```